--- a/src/build/.gitignore
+++ b/src/build/.gitignore
@@ -23,6 +23,11 @@ ciopfs
 /x64/
 /linux/debian_*-sysroot/
 /linux/ubuntu_*-sysroot/
+# We've built anything with pios in the name
+/linux/pios_*-sysroot/
+/linux/sysroot_scripts/*pios*
+/linux/sysroot_scripts/generated_package_lists/*pios*
+
 /ios_files
 /mac_files
 
--- a/src/build/linux/sysroot_scripts/generate_keyring.sh
+++ b/src/build/linux/sysroot_scripts/generate_keyring.sh
@@ -27,5 +27,19 @@ KEYS=(
     "4DFAB270CAA96DFA"
 )
 
+KEYFILE=keyring.gpg
+
+if [ $# -ge 1 ] && [ "$1" == "--pios" ]; then
+    KEYS+=(
+        # Raspbian
+        "A0DA38D0D76E8B5D638872819165938D90FDDD2E"
+        # Raspberry Pi OS
+        "CF8A1AF502A2AA2D763BAE7E82B129927FA3303E"
+    )
+    KEYFILE=pios_keyring.gpg
+    shift
+fi
+echo "Build keyring: ${SCRIPT_DIR}/${KEYFILE}"
+
 gpg --keyserver keyserver.ubuntu.com --recv-keys ${KEYS[@]}
-gpg --output "${SCRIPT_DIR}/keyring.gpg" --export ${KEYS[@]}
+gpg --output "${SCRIPT_DIR}/${KEYFILE}" --export ${KEYS[@]}
--- a/src/build/linux/sysroot_scripts/sysroot_creator.py
+++ b/src/build/linux/sysroot_scripts/sysroot_creator.py
@@ -7,6 +7,7 @@ This script is used to build Debian sysr
 """
 
 import argparse
+import gzip
 import hashlib
 import lzma
 import os
@@ -663,17 +664,19 @@ def create_tarball(install_root: str, ar
 
 
 def generate_package_list_dist_repo(arch: str, dist: str,
-                                    repo_name: str) -> list[dict[str, str]]:
-    repo_basedir = f"{ARCHIVE_URL}/dists/{dist}"
-    package_list = f"{BUILD_DIR}/Packages.{dist}_{repo_name}_{arch}"
-    package_list = f"{package_list}.{PACKAGES_EXT}"
-    package_file_arch = f"{repo_name}/binary-{arch}/Packages.{PACKAGES_EXT}"
+                                    repo_name: str, repo_src: str, repo_pre: str, repo_ext: str) -> list:
+    repo_basedir = f"{repo_src}/dists/{dist}"
+    package_list = f"{BUILD_DIR}/Packages.{repo_pre}{dist}_{repo_name}_{arch}"
+    package_list = f"{package_list}.{repo_ext}"
+    package_file_arch = f"{repo_name}/binary-{arch}/Packages.{repo_ext}"
     package_list_arch = f"{repo_basedir}/{package_file_arch}"
 
     download_or_copy_non_unique_filename(package_list_arch, package_list)
-    verify_package_listing(package_file_arch, package_list, dist)
+    verify_package_listing(package_file_arch, package_list, dist, repo_basedir, repo_pre)
 
-    with lzma.open(package_list, "rt") as src:
+    algo = lzma if repo_ext != "gz" else gzip
+
+    with algo.open(package_list, "rt") as src:
         return [
             dict(
                 line.split(": ", 1) for line in package_meta.splitlines()
@@ -682,11 +685,17 @@ def generate_package_list_dist_repo(arch
         ]
 
 
-def generate_package_list(arch: str) -> dict[str, str]:
+def generate_package_list(arch: str) -> dict:
     package_meta = {}
-    for dist, repos in APT_SOURCES_LIST:
+    for a in APT_SOURCES_LIST:
+        dist = a[0]
+        repos = a[1]
+        repo_src = ARCHIVE_URL if len(a) <= 2 else a[2]
+        repo_pre = "" if len(a) <= 3 else a[3] + "_"
+        repo_ext = PACKAGES_EXT if len(a) <= 4 else a[4]
         for repo_name in repos:
-            for meta in generate_package_list_dist_repo(arch, dist, repo_name):
+            for meta in generate_package_list_dist_repo(arch, dist, repo_name, repo_src, repo_pre, repo_ext):
+                meta["URL"] = repo_src
                 package_meta[meta["Package"]] = meta
 
     # Read the input file and create a dictionary mapping package names to URLs
@@ -697,7 +706,7 @@ def generate_package_list(arch: str) ->
         package = meta["Package"]
         if package in missing:
             missing.remove(package)
-            url = ARCHIVE_URL + meta["Filename"]
+            url = meta["URL"] + meta["Filename"]
             package_dict[url] = meta["SHA256"]
     if missing:
         raise Exception(f"Missing packages: {', '.join(missing)}")
@@ -783,7 +792,7 @@ def replace_in_file(file_path: str, sear
 
 
 def install_into_sysroot(build_dir: str, install_root: str,
-                         packages: dict[str, str]) -> None:
+                         packages: dict) -> None:
     """
     Installs libraries and headers into the sysroot environment.
     """
@@ -937,17 +946,16 @@ def upload_sysroot(arch: str) -> None:
 
 
 def verify_package_listing(file_path: str, output_file: str,
-                           dist: str) -> None:
+                           dist: str, repo_basedir:str, repo_pre: str) -> None:
     """
     Verifies the downloaded Packages.xz file against its checksum and GPG keys.
     """
     # Paths for Release and Release.gpg files
-    repo_basedir = f"{ARCHIVE_URL}/dists/{dist}"
     release_list = f"{repo_basedir}/{RELEASE_FILE}"
     release_list_gpg = f"{repo_basedir}/{RELEASE_FILE_GPG}"
 
-    release_file = os.path.join(BUILD_DIR, f"{dist}-{RELEASE_FILE}")
-    release_file_gpg = os.path.join(BUILD_DIR, f"{dist}-{RELEASE_FILE_GPG}")
+    release_file = os.path.join(BUILD_DIR, f"{repo_pre}{dist}-{RELEASE_FILE}")
+    release_file_gpg = os.path.join(BUILD_DIR, f"{repo_pre}{dist}-{RELEASE_FILE_GPG}")
 
     if not os.path.exists(KEYRING_FILE):
         raise Exception(f"KEYRING_FILE not found: {KEYRING_FILE}")
@@ -980,12 +988,35 @@ def verify_package_listing(file_path: st
 
 
 def main():
+    global APT_SOURCES_LIST
+    global RELEASE
+    global KEYRING_FILE
+
     parser = argparse.ArgumentParser(
         description="Build and upload Debian sysroot images for Chromium.")
     parser.add_argument("command", choices=["build", "upload"])
     parser.add_argument("architecture", choices=list(TRIPLES))
+    parser.add_argument("--pios", action="store_true")
     args = parser.parse_args()
 
+    if args.pios:
+        # Interleave our repos with those in the starting list
+        APT_PIOS_SOURCES_LIST = [
+            ("bookworm", ["main"], "http://archive.raspberrypi.org/debian/", "pios", "gz"),
+            ("bullseye", ["main"], "http://archive.raspberrypi.org/debian/", "pios", "gz")]
+        newlist = []
+        curdist = "xxx"
+        for a in APT_SOURCES_LIST:
+            if not a[0].startswith(curdist):
+                newlist += [p for p in APT_PIOS_SOURCES_LIST if p[0].startswith(curdist)]
+                curdist = a[0].partition('-')[0]
+            newlist += [a]
+        newlist += [p for p in APT_PIOS_SOURCES_LIST if p[0].startswith(curdist)]
+        APT_SOURCES_LIST = newlist
+
+        RELEASE = "pios-" + RELEASE
+        KEYRING_FILE = os.path.join(SCRIPT_DIR, "pios_keyring.gpg")
+
     sanity_check()
 
     if args.command == "build":
--- a/src/chrome/app/chrome_main_delegate.cc
+++ b/src/chrome/app/chrome_main_delegate.cc
@@ -74,6 +74,7 @@
 #include "components/nacl/common/buildflags.h"
 #include "components/startup_metric_utils/common/startup_metric_utils.h"
 #include "components/version_info/channel.h"
+#include "components/version_info/pi_patch_version_info.h"
 #include "components/version_info/version_info.h"
 #include "content/public/app/initialize_mojo_core.h"
 #include "content/public/common/content_client.h"
@@ -414,6 +415,15 @@ bool HandleVersionSwitches(const base::C
   }
 #endif
 
+  if (command_line.HasSwitch(switches::kPiPatchVersion)) {
+    printf("%s %s %s\nPi patch: %s\n",
+           version_info::GetProductName().data(),
+           version_info::GetVersionNumber().data(),
+           chrome::GetChannelName(chrome::WithExtendedStable(true)).c_str(),
+           version_info::GetPiPatchVersionString().c_str());
+    return true;
+  }
+
   if (command_line.HasSwitch(switches::kVersion)) {
     printf("%s %s %s\n", version_info::GetProductName().data(),
            version_info::GetVersionNumber().data(),
--- a/src/chrome/browser/about_flags.cc
+++ b/src/chrome/browser/about_flags.cc
@@ -7602,15 +7602,17 @@ const FeatureEntry kFeatureEntries[] = {
      FEATURE_VALUE_TYPE(ash::features::kArcInputOverlayAlphaV2)},
 #endif  // BUILDFLAG(IS_CHROMEOS_ASH)
 
-#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
+#if (BUILDFLAG(IS_CHROMEOS) || BUILDFLAG(IS_LINUX)) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
 #if !BUILDFLAG(USE_VAAPI)
     {"chromeos-direct-video-decoder",
      flag_descriptions::kChromeOSDirectVideoDecoderName,
      flag_descriptions::kChromeOSDirectVideoDecoderDescription,
-     kOsCrOS | kOsLacros,
+     kOsCrOS | kOsLacros | kOsLinux,
      FEATURE_VALUE_TYPE(media::kUseChromeOSDirectVideoDecoder)},
 #endif  // !BUILDFLAG(USE_VAAPI)
+#endif
 
+#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
     {"enable-vbr-encode-acceleration",
      flag_descriptions::kChromeOSHWVBREncodingName,
      flag_descriptions::kChromeOSHWVBREncodingDescription, kOsCrOS | kOsLacros,
--- a/src/chrome/browser/flag_descriptions.cc
+++ b/src/chrome/browser/flag_descriptions.cc
@@ -7660,7 +7660,8 @@ const char kWebPrintingApiDescription[]
     "https://github.com/WICG/web-printing for details.";
 #endif  // BUILDFLAG(IS_CHROMEOS)
 
-#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
+// RPI: Allow Direct decode under linux as well as chromeos
+#if (BUILDFLAG(IS_CHROMEOS) || BUILDFLAG(IS_LINUX)) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
 #if !BUILDFLAG(USE_VAAPI)
 const char kChromeOSDirectVideoDecoderName[] = "ChromeOS Direct Video Decoder";
 const char kChromeOSDirectVideoDecoderDescription[] =
@@ -7670,6 +7671,9 @@ const char kChromeOSDirectVideoDecoderDe
     "disable_cros_video_decoder USE flag in ChromeOS). This flag only has an "
     "effect on non-Intel and non-AMD devices (i.e. on ARM-based SoCs).";
 #endif  // !BUILDFLAG(USE_VAAPI)
+#endif
+// RPI: Reset to upstream compile options
+#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
 const char kChromeOSHWVBREncodingName[] =
     "ChromeOS Hardware Variable Bitrate Encoding";
 const char kChromeOSHWVBREncodingDescription[] =
--- a/src/chrome/browser/flag_descriptions.h
+++ b/src/chrome/browser/flag_descriptions.h
@@ -4444,9 +4444,13 @@ extern const char kAudioFlexibleLoopback
 extern const char kAudioFlexibleLoopbackForSystemLoopbackDescription[];
 #endif  // BUILDFLAG(IS_CHROMEOS)
 
-#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
+// RPI: Allow Direct decode under linux as well as chromeos
+#if (BUILDFLAG(IS_CHROMEOS) || BUILDFLAG(IS_LINUX)) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
 extern const char kChromeOSDirectVideoDecoderName[];
 extern const char kChromeOSDirectVideoDecoderDescription[];
+#endif
+// RPI: Reset to upstream compile options
+#if BUILDFLAG(IS_CHROMEOS) && BUILDFLAG(USE_CHROMEOS_MEDIA_ACCELERATION)
 extern const char kChromeOSHWVBREncodingName[];
 extern const char kChromeOSHWVBREncodingDescription[];
 #if defined(ARCH_CPU_ARM_FAMILY)
--- a/src/chrome/common/chrome_paths.cc
+++ b/src/chrome/common/chrome_paths.cc
@@ -410,6 +410,10 @@ bool PathProvider(int key, base::FilePat
 
 #if BUILDFLAG(ENABLE_WIDEVINE)
     case chrome::DIR_BUNDLED_WIDEVINE_CDM:
+      cur = base::FilePath(FILE_PATH_LITERAL("/opt"));
+      cur = cur.Append(kWidevineCdmBaseDirectory);
+      if (base::PathExists(cur))
+        break;
       if (!GetComponentDirectory(&cur)) {
         return false;
       }
--- a/src/chrome/common/chrome_switches.cc
+++ b/src/chrome/common/chrome_switches.cc
@@ -463,6 +463,9 @@ const char kPackExtensionKey[] = "pack-e
 // crashpad (or breakpad) is initialized.
 const char kPreCrashpadCrashTest[] = "pre-crashpad-crash-test";
 
+// Print the patch version and return
+const char kPiPatchVersion[] = "pi-patch-version";
+
 // Used to mock the response received from the Web Permission Prediction
 // Service. Used for testing.
 const char kPredictionServiceMockLikelihood[] =
--- a/src/chrome/common/chrome_switches.h
+++ b/src/chrome/common/chrome_switches.h
@@ -149,6 +149,7 @@ extern const char kOnTheFlyMhtmlHashComp
 extern const char kOpenInNewWindow[];
 extern const char kPackExtension[];
 extern const char kPackExtensionKey[];
+extern const char kPiPatchVersion[];
 extern const char kPreCrashpadCrashTest[];
 extern const char kPredictionServiceMockLikelihood[];
 extern const char kPreinstalledWebAppsDir[];
--- a/src/chrome/common/media/cdm_registration.cc
+++ b/src/chrome/common/media/cdm_registration.cc
@@ -58,9 +58,7 @@ namespace {
 using Robustness = content::CdmInfo::Robustness;
 
 #if BUILDFLAG(ENABLE_WIDEVINE)
-#if (BUILDFLAG(BUNDLE_WIDEVINE_CDM) ||            \
-     BUILDFLAG(ENABLE_WIDEVINE_CDM_COMPONENT)) && \
-    (BUILDFLAG(IS_LINUX) || BUILDFLAG(IS_CHROMEOS))
+#if BUILDFLAG(IS_LINUX) || BUILDFLAG(IS_CHROMEOS)
 // Create a CdmInfo for a Widevine CDM, using |version|, |cdm_library_path|, and
 // |capability|.
 std::unique_ptr<content::CdmInfo> CreateWidevineCdmInfo(
@@ -104,7 +102,7 @@ std::unique_ptr<content::CdmInfo> Create
         // BUILDFLAG(ENABLE_WIDEVINE_CDM_COMPONENT)) && (BUILDFLAG(IS_LINUX) ||
         // BUILDFLAG(IS_CHROMEOS))
 
-#if BUILDFLAG(BUNDLE_WIDEVINE_CDM) && \
+#if BUILDFLAG(ENABLE_WIDEVINE) && \
     (BUILDFLAG(IS_LINUX) || BUILDFLAG(IS_CHROMEOS))
 // On Linux/ChromeOS we have to preload the CDM since it uses the zygote
 // sandbox. On Windows and Mac, CDM registration is handled by Component
@@ -207,7 +205,7 @@ void AddSoftwareSecureWidevine(std::vect
   // We should still use the version selected by Component Update (except for
   // case #3 above).
   content::CdmInfo* bundled_widevine = nullptr;
-#if BUILDFLAG(BUNDLE_WIDEVINE_CDM)
+#if BUILDFLAG(ENABLE_WIDEVINE)
   bundled_widevine = GetBundledWidevine();
 #endif
 
--- a/src/components/version_info/BUILD.gn
+++ b/src/components/version_info/BUILD.gn
@@ -10,6 +10,9 @@ source_set("version_info") {
   sources = [
     "version_info.h",
     "version_info_values.h",
+    "pi_patch_version_info.cc",
+    "pi_patch_version_info.h",
+    "pi_patch_version_values.h",
   ]
   public_deps = [
     ":channel",
--- /dev/null
+++ b/src/components/version_info/pi_patch_version_info.cc
@@ -0,0 +1,11 @@
+#include "components/version_info/pi_patch_version_info.h"
+#include "components/version_info/pi_patch_version_values.h"
+
+namespace version_info {
+
+std::string GetPiPatchVersionString() {
+  return PI_PATCH_VERSION_STRING;
+}
+
+}  // namespace version_info
+
--- /dev/null
+++ b/src/components/version_info/pi_patch_version_info.h
@@ -0,0 +1,12 @@
+#ifndef COMPONENTS_PI_PATCH_VERSION_INFO_VERSION_INFO_H_
+#define COMPONENTS_PI_PATCH_VERSION_INFO_VERSION_INFO_H_
+
+#include <string>
+
+namespace version_info {
+
+// Returns a string with the patch tag for our patches
+std::string GetPiPatchVersionString();
+
+}  // namespace version_info
+#endif  // COMPONENTS_VERSION_INFO_VERSION_INFO_H_
--- /dev/null
+++ b/src/components/version_info/pi_patch_version_values.h
@@ -0,0 +1,2 @@
+// Pi patch version - generated by pi-util/settag.py
+#define PI_PATCH_VERSION_STRING "rpi_3.67"
--- a/src/content/common/user_agent.cc
+++ b/src/content/common/user_agent.cc
@@ -63,7 +63,7 @@ std::string GetUserAgentPlatform() {
 std::string GetUnifiedPlatform() {
 #if BUILDFLAG(IS_ANDROID)
   return "Linux; Android 10; K";
-#elif BUILDFLAG(IS_CHROMEOS)
+#elif BUILDFLAG(IS_CHROMEOS) || BUILDFLAG(IS_POSIX)
   return "X11; CrOS x86_64 14541.0.0";
 #elif BUILDFLAG(IS_MAC)
   return "Macintosh; Intel Mac OS X 10_15_7";
@@ -252,7 +252,7 @@ std::string GetOSVersion(IncludeAndroidB
                       "%s%s", android_version_str.c_str(),
                       android_info_str.c_str()
 #else
-                      ""
+                      "13597.84.0"
 #endif
   );
   return os_version;
@@ -287,7 +287,7 @@ std::string BuildOSCpuInfoFromOSVersionA
   base::StringAppendF(&os_cpu,
 #if BUILDFLAG(IS_MAC)
                       "%s Mac OS X %s", cpu_type.c_str(), os_version.c_str()
-#elif BUILDFLAG(IS_CHROMEOS)
+#elif BUILDFLAG(IS_CHROMEOS) || BUILDFLAG(IS_POSIX)
                       "CrOS "
                       "%s %s",
                       cpu_type.c_str(),  // e.g. i686
--- a/src/media/gpu/BUILD.gn
+++ b/src/media/gpu/BUILD.gn
@@ -21,6 +21,7 @@ buildflag_header("buildflags") {
     "USE_VAAPI=$use_vaapi",
     "USE_VAAPI_IMAGE_CODECS=$use_vaapi_image_codecs",
     "USE_V4L2_CODEC=$use_v4l2_codec",
+    "USE_V4L2_CODEC_RPI=$use_v4l2_codec_rpi",
     "USE_LIBV4L2=$use_v4lplugin",
   ]
 }
--- a/src/media/gpu/args.gni
+++ b/src/media/gpu/args.gni
@@ -14,6 +14,10 @@ declare_args() {
   use_v4l2_codec =
       is_chromeos_lacros && (target_cpu == "arm" || target_cpu == "arm64")
 
+  # Indicates if this is V4L2 on RPi.  Only compiles stateful V4L2 code
+  # and removes all legacy codecs
+  use_v4l2_codec_rpi = false
+
   # Indicates if VA-API-based hardware acceleration is to be used. This
   # is typically the case on x86-based ChromeOS devices.
   # VA-API should also be compiled by default on x11/wayland linux devices
--- a/src/media/gpu/chromeos/fourcc.cc
+++ b/src/media/gpu/chromeos/fourcc.cc
@@ -34,6 +34,7 @@ std::optional<Fourcc> Fourcc::FromUint32
     case MM21:
     case P010:
     case MT2T:
+    case BGR4:
     case AR24:
     case Q08C:
     case Q10C:
@@ -188,6 +189,7 @@ VideoPixelFormat Fourcc::ToVideoPixelFor
       return PIXEL_FORMAT_P016LE;
     case MT2T:
       return PIXEL_FORMAT_P016LE;
+    case BGR4:
     case AR24:
       return PIXEL_FORMAT_ARGB;
     // V4L2_PIX_FMT_QC08C is a proprietary Qualcomm compressed format that can
@@ -292,6 +294,7 @@ std::optional<Fourcc> Fourcc::ToSinglePl
     case P010:
     case MM21:
     case MT2T:
+    case BGR4:
     case AR24:
       return Fourcc(value_);
     case YM12:
@@ -327,6 +330,7 @@ bool Fourcc::IsMultiPlanar() const {
     case YU16:
     case P010:
     case MT2T:
+    case BGR4:
     case AR24:
     case Q08C:
     case Q10C:
--- a/src/media/gpu/chromeos/fourcc.h
+++ b/src/media/gpu/chromeos/fourcc.h
@@ -100,6 +100,9 @@ class MEDIA_GPU_EXPORT Fourcc {
 
     // Single plane 8-bit little-endian ARGB (bytes in reverse B-G-R-A order).
     AR24 = ComposeFourcc('A', 'R', '2', '4'),
+    // Single plane 8-bit little-endian XRGB (bytes in reverse B-G-R-X order).
+    BGR4 = ComposeFourcc('B', 'G', 'R', '4'),
+
     // V4L2 proprietary format.
     // https://linuxtv.org/downloads/v4l-dvb-apis-new/userspace-api/v4l/pixfmt-reserved.html
     // Opaque format that can only be scanned out as an overlay or composited by
--- a/src/media/gpu/v4l2/v4l2_device.cc
+++ b/src/media/gpu/v4l2/v4l2_device.cc
@@ -913,11 +913,19 @@ void V4L2Device::CloseDevice() {
 }
 
 void V4L2Device::EnumerateDevicesForType(Type type) {
+#if BUILDFLAG(USE_V4L2_CODEC_RPI)
+  static const std::string kDecoderDevicePattern = "/dev/video";
+  static const std::string kEncoderDevicePattern = "/dev/video";
+  static const std::string kImageProcessorDevicePattern = "/dev/video";
+  static const std::string kJpegDecoderDevicePattern = "/dev/video";
+  static const std::string kJpegEncoderDevicePattern = "/dev/video";
+#else
   static const std::string kDecoderDevicePattern = "/dev/video-dec";
   static const std::string kEncoderDevicePattern = "/dev/video-enc";
   static const std::string kImageProcessorDevicePattern = "/dev/image-proc";
   static const std::string kJpegDecoderDevicePattern = "/dev/jpeg-dec";
   static const std::string kJpegEncoderDevicePattern = "/dev/jpeg-enc";
+#endif
 
   std::string device_pattern;
   v4l2_buf_type buf_type;
@@ -953,7 +961,11 @@ void V4L2Device::EnumerateDevicesForType
   // We are sandboxed, so we can't query directory contents to check which
   // devices are actually available. Try to open the first 10; if not present,
   // we will just fail to open immediately.
+#if BUILDFLAG(USE_V4L2_CODEC_RPI)
+  for (int i = 10; i < 35; ++i) {
+#else
   for (int i = 0; i < 10; ++i) {
+#endif
     candidate_paths.push_back(
         base::StringPrintf("%s%d", device_pattern.c_str(), i));
   }
--- a/src/media/gpu/v4l2/v4l2_utils.cc
+++ b/src/media/gpu/v4l2/v4l2_utils.cc
@@ -500,7 +500,13 @@ struct timeval TimeDeltaToTimeVal(base::
 std::optional<SupportedVideoDecoderConfigs> GetSupportedV4L2DecoderConfigs() {
   SupportedVideoDecoderConfigs supported_media_configs;
 
+#if !BUILDFLAG(USE_V4L2_CODEC_RPI)
   constexpr char kVideoDeviceDriverPath[] = "/dev/video-dec0";
+#else
+  // Pi stateful decoder here  current google code doesn't look like it can
+  // cope with multiple devioces
+  constexpr char kVideoDeviceDriverPath[] = "/dev/video10";
+#endif
   base::ScopedFD device_fd(HANDLE_EINTR(
       open(kVideoDeviceDriverPath, O_RDWR | O_NONBLOCK | O_CLOEXEC)));
   if (!device_fd.is_valid()) {
@@ -546,7 +552,13 @@ std::optional<SupportedVideoDecoderConfi
 }
 
 bool IsV4L2DecoderStateful() {
+#if !BUILDFLAG(USE_V4L2_CODEC_RPI)
   constexpr char kVideoDeviceDriverPath[] = "/dev/video-dec0";
+#else
+  // Pi stateful decoder here  current google code doesn't look like it can
+  // cope with multiple devioces
+  constexpr char kVideoDeviceDriverPath[] = "/dev/video10";
+#endif
   base::ScopedFD device_fd(HANDLE_EINTR(
       open(kVideoDeviceDriverPath, O_RDWR | O_NONBLOCK | O_CLOEXEC)));
   if (!device_fd.is_valid()) {
--- a/src/media/gpu/v4l2/v4l2_video_decoder.cc
+++ b/src/media/gpu/v4l2/v4l2_video_decoder.cc
@@ -391,10 +391,12 @@ V4L2Status V4L2VideoDecoder::InitializeB
   DVLOGF(3);
   DCHECK_CALLED_ON_VALID_SEQUENCE(decoder_sequence_checker_);
 
+  const int instances = num_instances_.Increment();
   can_use_decoder_ =
-      num_instances_.Increment() < kMaxNumOfInstances ||
+      instances < kMaxNumOfInstances ||
       !base::FeatureList::IsEnabled(media::kLimitConcurrentDecoderInstances);
   if (!can_use_decoder_) {
+    num_instances_.Decrement();
     VLOGF(1) << "Reached maximum number of decoder instances ("
              << kMaxNumOfInstances << ")";
     return V4L2Status::Codes::kMaxDecoderInstanceCount;
--- a/src/media/gpu/v4l2/v4l2_video_decoder.h
+++ b/src/media/gpu/v4l2/v4l2_video_decoder.h
@@ -186,7 +186,12 @@ class MEDIA_GPU_EXPORT V4L2VideoDecoder
   // decoder instances for now. |num_instances_| tracks the number of
   // simultaneous decoders. |can_use_decoder_| is true iff we haven't reached
   // the maximum number of instances at the time this decoder is created.
+#if BUILDFLAG(USE_V4L2_CODEC_RPI)
+  // Pi can run out of firmware memory very quickly
+  static constexpr int kMaxNumOfInstances = 2;
+#else
   static constexpr int kMaxNumOfInstances = 32;
+#endif
   static base::AtomicRefCount num_instances_;
   bool can_use_decoder_ = false;
 
--- a/src/media/gpu/v4l2/v4l2_video_decoder_backend_stateful.cc
+++ b/src/media/gpu/v4l2/v4l2_video_decoder_backend_stateful.cc
@@ -85,6 +85,7 @@ V4L2StatefulVideoDecoderBackend::V4L2Sta
   DVLOGF(3);
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   weak_this_ = weak_this_factory_.GetWeakPtr();
+  LOG(INFO) << __func__;
 }
 
 V4L2StatefulVideoDecoderBackend::~V4L2StatefulVideoDecoderBackend() {
@@ -101,6 +102,7 @@ V4L2StatefulVideoDecoderBackend::~V4L2St
   if (device_->Ioctl(VIDIOC_UNSUBSCRIBE_EVENT, &sub) != 0) {
     VLOGF(1) << "Cannot unsubscribe to event";
   }
+  LOG(INFO) << __func__;
 }
 
 bool V4L2StatefulVideoDecoderBackend::Initialize() {
@@ -590,6 +592,7 @@ void V4L2StatefulVideoDecoderBackend::On
 void V4L2StatefulVideoDecoderBackend::ChangeResolution() {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DVLOGF(3);
+  LOG(INFO) << __func__;
 
   // Here we just query the new resolution, visible rect, and number of output
   // buffers before asking the client to update the resolution.
--- a/src/media/mojo/services/gpu_mojo_media_client_linux.cc
+++ b/src/media/mojo/services/gpu_mojo_media_client_linux.cc
@@ -54,6 +54,7 @@ std::vector<Fourcc> GetPreferredRenderab
 
   // Support 1-copy argb textures.
   renderable_fourccs.emplace_back(Fourcc::AR24);
+  renderable_fourccs.emplace_back(Fourcc::BGR4);
 
   return renderable_fourccs;
 }
--- /dev/null
+++ b/src/pi-util/BUILD.txt
@@ -0,0 +1,256 @@
+Build notes (cross compile from Ubuntu)
+=======================================
+
+Build from a patch
+------------------
+
+# Pick somewhere to put this
+cd ~
+mkdir chromium
+cd chromium
+# Get the build tools & put on path
+# You may want to add the path in .bashrc
+git clone https://chromium.googlesource.com/chromium/tools/depot_tools
+export PATH=$PATH:`pwd`/depot_tools
+# Get the main tree
+fetch chromium
+cd src
+# Checkout the version you want
+# * Fix version number
+git checkout 55.0.2883.99
+# Fix up any missing dependancies on the build m/c
+# * may well be unnecessary if you have built any other chrome
+./build/install-build-deps.sh
+./build/install-build-deps.sh --arm
+# Fetch & pull the other bits of the tree to sync.
+# As we are checking out a tag the --with_branch_heads is important
+gclient sync --with_branch_heads
+# Patch - should be completely clean if everything matchs
+# * Fix patch file to correct name / location
+cd ..
+patch -p1 < v55.0.2883.99.patch
+cd src
+# * Get a sysroot from somewhere and put it in build/linux/raspian_jessie_pi1-sysroot
+# * Example below is only if you have got an appropriate one lying around
+# * Otherwise follow sysroot instructions further down
+rsync -rl previous_location/raspian_jessie_pi1-sysroot build/linux/
+# Build output directories (out/armv6, out/armv7)
+# * This script currently assumes a sysroot of build/linux/raspian_jessie_pi1-sysroot
+#   so may need editing if you have put it elsewhere
+pi-util/gngen.py
+# Build chrome
+ninja -C out/armv6 chrome chrome_sandbox
+# Build armv7 ffmpeg
+ninja -C out/armv7 third_party/ffmpeg
+
+
+To run on a Pi
+--------------
+
+This requires a little installation.  The sandbox and ffmpeg shared libs
+need to be copied to the pi.  As neither is being tweaked much by me these
+steps should only be required if the underlying Chrome changes.  Otherwise
+you can just run out of the build directory (src/out/armv6)
+
+Assuming you can mount the build dir from the pi.
+
+# On the Pi NOT the build machine
+cd <path to build env>/src
+# Copy the ffmpeg libs
+pi-util/cplibs.sh
+# Copy the sandbox. BUILDTYPE tells the script where to get it from
+# This doesn't seem to be needed anymore with linux 4.9 and chrome 55
+BUILDTYPE=armv6 build/update-linux-sandbox.sh
+# Run chrome
+cd out/armv6
+./chrome
+
+
+Rebuilds
+--------
+
+In most cases a simple "ninja -C out/armv6 chrome" is all that is needed
+and the pi can run from out/armv6.
+
+To clean build "rm -rf out" and follow the build instructions from gngen.py
+
+
+Updating chromium from git
+--------------------------
+
+There is no script for this as the merges are prone to conflicts and it
+is much easier to sort them if you are doing stuff manually.
+
+If updating between major versions then mergeing tends to fail horribly
+so something along the lines of:
+
+# Look for where we are going
+git tag -l "118.*"
+TAG=118...
+
+# Remember where we are
+OLDTAG=`sed -nE 's/^src[^0-9]*([0-9.]+)"$/\1/p' pi-util/pipaths.py`
+OLDVER=`echo $OLDTAG | sed -nE 's/^([0-9]+).*$/\1/p'`
+echo OLDTAG=$OLDTAG OLDVER=$OLDVER
+
+NEWVER=`echo $TAG | sed -nE 's/^([0-9]+).*$/\1/p'`
+echo NEWTAG=$TAG NEWVER=$NEWVER
+
+# * Make sure there are no updates required and no untracked files
+pi-util/gitscan.py status
+# Set rename limit to huge as files are moved around frequently and getting
+# git to track them is much easier than trying to do it ourselves
+git config diff.renameLimit 1000000
+
+# Tag source & make a patch file - patch file is useful when files are moved
+# as then git goes all unhelpful
+pi-util/dodiff.py > ../v${OLDTAG}_stash.patch
+# As git stash will reset the branch switch to a temp branch 1st
+pi-util/gitscan.py checkout -b stash/${OLDVER}/base
+pi-util/gitscan.py --gitscan-no-src reset {BASE}
+pi-util/gitscan.py --gitscan-no-src stash -u
+pi-util/gitscan.py --gitscan-no-src checkout {BASE}
+
+# Need to do src separately as the stash will lose pi-utils
+git reset $OLDTAG
+git stash -u
+
+git checkout $TAG -b test/${NEWVER}/rpi_2
+
+# Clean out old objects
+rm -rf out
+# Beware that git clean might kill our sub-repos so so don't do it unless we
+# are sure it won't
+# git clean -dxf
+
+### Do the "get environment" stages of a new build
+./build/install-build-deps.sh
+./build/install-build-deps.sh --arm
+# Fetch & pull the other bits of the tree to sync.
+# As we are checking out a tag the --with_branch_heads is important
+gclient sync -D --with_branch_heads
+
+# Start rebuild
+git stash pop
+
+# Fix pipaths & make new branches (now so we don't forget later)
+sed "s/src_commit=.*/src_commit=\"$TAG\"/" pi-util/pipaths.py | tee t
+mv t pi-util/pipaths.py && git add pi-util/pipaths.py
+chmod 0755 pi-util/*.py pi-util/*.sh
+pi-util/gitscan.py --gitscan-no-src checkout -b test/${NEWVER}/rpi_2
+
+# If running with filemode false then readd pi-util with filemode true
+git config core.filemode true
+git add pi-util
+git config core.filemode false
+
+
+### Fix conflicts (there will be some)
+
+# If building a separated ffmpeg .so (we are not not currrently) then
+# fix chrome major version for ffmpeg .so in pi-util/cplibs.sh and third_party/ffmpeg/BUILD.gn
+
+git commit
+### run through all other dirs we care about doing the same
+### Probably need to fix exec perms on pi-util scripts
+pi-util/rootgen.sh
+pi-util/gngen.py
+### Fix up any new libpackage-dev that we need
+ninja ...
+### Fix up build disasters
+
+
+If updating within a major version mergeing seems to work reliably so my
+preferred method for achieving this goes:
+
+# Make sure everything is committed
+pi-util/gitscan.py status
+# Revert to base chromium checkout for old checkout
+pi-util/gitscan.py --gitscan-no-src checkout {BASE}
+# Merge new version into current base
+git fetch --all
+TAG=<new_tag>
+NEWVER=`echo $TAG | sed -nE 's/^([0-9]+).*$/\1/p'`
+
+git merge $TAG
+# Fix conflicts - DEPS always seems to conflict
+git checkout $TAG -- DEPS
+# Update pi-util/pipaths.py to contain the new tag
+# Either commit now or later
+sed "s/src_commit=.*/src_commit=\"$TAG\"/" pi-util/pipaths.py | tee t
+mv t pi-util/pipaths.py && git add pi-util/pipaths.py
+git commit --no-edit
+# Get the rest of the tree
+gclient sync --with_branch_heads
+# Checkout our tree and merge the new base into it
+pi-util/gitscan.py --gitscan-no-src checkout test/$NEWVER/rpi_2
+pi-util/gitscan.py --gitscan-no-src merge --no-edit {BASE}
+
+and we should be good to go.  At this point you can either clean build or
+not.  Chromes dependancy checks seem remarkably good so a simple build
+works nearly all the time.
+
+# Rebuild from clean
+rm -rf out
+# refetch root (optional)
+pi-util/rootgen.sh
+# Configure
+pi-util/gngen.py
+# Build release armv7 chrome (and any other targets you feel like)
+ninja -C out/armv7-rel chrome
+
+
+Sysroots (one time only)
+------------------------
+
+1st you will need to get the dev files for a bunch of libs on your pi (or
+if you can get the right files by magic on your cross-compile m/c then
+that is good too).  In src/pi-util there is a shell script
+pi-install-dev.sh which lists all the libs I think are needed along with a
+helpful apt-get install so all you should need to do is run it on an
+appropriate pi.
+
+Next the appropriate bits need to be copied to
+build/linux/<sysroot-name>-sysroot. We use raspian_stretch_pi1 as the
+sysroot name in these instructions and in the example script files so you
+might well find it easiest to use the same name too
+
+The script pi-util/syncroot.sh that will copy the needed bits of a root to
+the right place and then fix the full path symlinks to be relative.  It
+uses rsync to copy the files so the src can contain a machine name
+
+pi-util/syncroot.sh my-pi: raspian_stretch_pi1
+
+The "raspian_stretch_pi1" can be omitted and syncroot will choose the current
+default sysroot name.
+
+Beware that there are ~8 rsync statements so if the rsync is operating
+over ssh then you may need to type your password 8 times...  Note also
+that the script appends -sysroot to the given name so don't add that
+yourself!
+
+If the pi root is updated then this script can / should be rerun to update
+the sysroot.
+
+
+
+Other notes on the tree
+-----------------------
+
+The definitive list of expected repos is in pi-util/pipaths.py
+
+The script pi-util/gitscan.py will perform the same git op on all the
+repos that are in use in the current patch set.  It has substitutions
+of {PATH} and {BASE} for the path to the current repo and the chromium
+commit on which the current branch is based
+
+The current dev branch is test/57/mmal_2
+
+Status of optional neon by build file:
+skia/BUILD.gn:                     yes
+build/secondary/third_party/libjpeg_turbo/BUILD.gn: yes
+third_party/libwebp/BUILD.gn:      yes
+third_party/openmax_dl/dl/BUILD.gn unused
+third_party/libyuv/BUILD.gn:       yes
+third_party/libyuv/libyuv.gni:     yes
+third_party/pdfium/skia/BUILD.gn:  unused
--- /dev/null
+++ b/src/pi-util/README.txt
@@ -0,0 +1,118 @@
+Release notes
+=============
+
+This version should run with gpu-mem=64 with the default switches. Having
+said that this will only allow for 1 stream.  If you are playing >1 stream
+(even transiently) then you will need more (say gpu_mem=128) and you will
+need to set the --mmal-decoders option to the desired max number. The code
+should give up cleanly if it cannot allocate a h/w video decoder and give
+the stream to old-style ffmpeg decode, but as it stands in many cases it
+thinks it has allocated a decoder cleanly only to find that it fails when
+it tries to use it.
+
+Needs a current (buster 2019-06-07+) firmware/userland
+
+There are a few command-line switches - in general you shouldn't use
+them!
+
+
+Decode and resizer options
+--------------------------
+
+--mmal-decode-opaque     Set the decoder to use opaque frames between
+decoder and resizer.  This should be faster than i420 but doesn't work
+with old firmware.  This is the default with newer firmware (>=
+2016-11-01). (see --mmal-decode-i420)
+
+--mmal-decode-i420       Set the decoder to use I420 frames between
+decoder and resizer.  This generates an unnecessary conversion but works
+with all firmware.  This is the default with older firmware (<
+2016-11-01). (see --mmal-decode-opaque)
+
+--mmal-low-delay         Force "low-delay" mode on the decoder pipe.  This
+reduces the number of buffered ES frames before the decoder.  It isn't
+exactly low-delay but is definitely lower than otherwise.  May have a
+slight performance penalty and increase the risk of stuttering.  This mode
+will be automatically set by Chrome for some streams.
+
+--mmal-resize-isp        Use ISP resize rather than resizer.  Is noticably
+faster but requires --mmal-frame-copy or --mmal-zero-copy and newer
+firmware.  This is the default with newer firmware  (>= 2016-11-01) and
+enough gpu memory to support --mmal-frame-copy.
+
+--mmal-resize-resizer    Use resizer rather than ISP. Slower than ISP
+resize but supports older firmware and --mmal-slice-copy which may be
+needed if GPU memory is very limited (as will be the case on a Pi1 with a
+default setup).
+
+--mmal-resize-mode=NEVER|ALWAYS|SMALLER
+Sets resize behaviour.
+  NEVER    Output is the native size of the video
+  ALWAYS   Output allways attempts to match the size of the displayed picture
+           This is normally the fastest mode for SHM-RGB copy
+  SMALLER  Resize to smaller of native & display. This saves memory and is
+           the fastest for EGL output
+
+
+Copy-modes
+----------
+
+--mmal-copy-mode=<copy mode>
+
+This sets the output frame type & mmal->chrome copy mode. Current values
+for <copy mode> are:
+
+slice                   slowest - uses only a small amount of memory
+                        in the resizer
+
+<alloc>-<format>-<copy>
+  <alloc>
+    SHM      Frame allocated from shared memory
+    GPU      Frame allocated from gpu memory
+  <format>
+    YUV      3-plane I420
+    YC       2-plane I420 e.g. NV12
+    RGB      1-plane 4-byte RGBX
+  <copy>
+    COPY     Data copied on the ARM.  This should be slower than DMA but
+             sometimes give more performance at the expense of slightly
+	     higher ARM CPU usage
+    DMA      Data copied by firmware DMA to ARM buffers.
+    ZC       Data put directly into GPU buffer.  Fastest - only works
+             with EGL (needs vcsm-cma).
+
+Currently valid combinations are:
+
+SHM-YUV-DMA
+SHM-YC-DMA
+SHM-RGB-DMA
+SHM-RGB-COPY  Default for non-gpu operation
+GPU-RGB-DMA
+GPU-RGB-ZC
+GPU-YUV-COPY
+GPU-YUV-ZC    Default for EGL operation
+
+
+Misc options
+------------
+
+--enable-logging=stderr This is a standard option for chrome but worth
+noting as the mmal code will print out its interpretation of the command
+line options passed to it along with how much GPU memory it has detected
+and the firmware date.
+
+--pi-patch-version       Print out the versions of Chromium and Pi
+patches.  Chrome will then terminate
+
+--mmal-decoders=<n>      Set the number of mmal decoders we wil try to
+create simultainiously. Default=1. If this number is exceeded then decoder
+init will fail and chrome will fallback to ffmpeg decode.  There is no
+panalty for setting this to a large number if you wish to have "unlimited"
+decoders.  However if it is set too big and there isn't the gpu mem to
+satisfy the requirements of the decode it may fail cleanly and revert to
+software (ffmpeg) decode or init may appear to succeed and decode then
+fails in an undefined manner.
+
+--mmal-frame-buffers=<n> Set the number of gpu "frame" buffers.
+Change with care.
+
--- /dev/null
+++ b/src/pi-util/cpbuild.sh
@@ -0,0 +1,31 @@
+set -e
+if [ "$2" == "" ]; then
+  echo "mkzip <zipname> <out/dir>"
+  exit 1
+fi
+
+BASEDIR=`pwd`
+TMPBASE=$BASEDIR/out/tmp
+TMPDIRNAME=$1
+ZIPFILE=$1.zip
+OUTDIR=$BASEDIR/$2
+
+cd $OUTDIR
+D=$TMPBASE/$TMPDIRNAME
+rm -rf $D
+mkdir -p $D
+
+echo "=== Copying"
+cp -r * $D
+cd $D
+
+echo "=== Clean unwanted"
+find . -name obj -exec rm -rf {} +
+rm -rf gen clang_*
+rm -rf *.TOC *_deps *.zip core-* bin test_* toolchain.ninja third_party tools local_rustc_sysroot thinlto-cache
+cd $TMPBASE
+
+echo "=== Zipping"
+zip -r -q $ZIPFILE $TMPDIRNAME
+
+echo "=== Done: $TMPBASE/$ZIPFILE"
--- /dev/null
+++ b/src/pi-util/cplibs.sh
@@ -0,0 +1,19 @@
+set -e
+
+FFNAME=libffmpeg_chrome.so.66
+LIBROOT=/usr/lib/arm-linux-gnueabihf
+
+if [ ! -d $LIBROOT ]; then
+  echo Can\'t find $LIBROOT
+  echo Are you sure you are running this on a Pi?
+  exit 1
+fi
+
+echo Copying $FFNAME from armv6/7 to $LIBROOT/...
+
+cp out/armv7/$FFNAME /tmp
+sudo cp /tmp/$FFNAME $LIBROOT/neon/vfp
+cp out/armv6/$FFNAME /tmp
+sudo cp /tmp/$FFNAME $LIBROOT
+
+
--- /dev/null
+++ b/src/pi-util/defargs_arm64-bullseye.gn
@@ -0,0 +1,33 @@
+# Build arguments go here. Examples:
+#   is_component_build = true
+#   is_debug = false
+# See "gn args <out_dir> --list" for available build arguments.
+target_cpu = "arm64"
+target_os = "linux"
+
+# Enables screen sharing in hangouts
+enable_hangout_services_extension = true
+
+ffmpeg_branding = "Chrome"
+proprietary_codecs = true
+
+enable_widevine = true
+
+# Dawn seems to really want Vulkan - Chrome doesn't like our Vulkan
+# this causes a hiccup about a min after startup.
+# Disabling Dawn seems to create no penalty but this probably won't
+# be true indefinitely
+use_dawn = false
+skia_use_dawn = false
+
+# We don't have the required GL extensions to enable GL passthrough so must
+# enable the validating decode otherwise we get a fatal GPU error
+enable_validating_command_decoder = true
+
+disable_fieldtrial_testing_config = true
+enable_nacl = false
+blink_symbol_level = 0
+symbol_level = 1
+is_official_build = true
+is_component_ffmpeg = true
+
--- /dev/null
+++ b/src/pi-util/defargs_armv7-bullseye.gn
@@ -0,0 +1,41 @@
+# Build arguments go here. Examples:
+#   is_component_build = true
+#   is_debug = false
+# See "gn args <out_dir> --list" for available build arguments.
+target_cpu = "arm"
+target_os = "linux"
+
+arm_float_abi = "hard"
+arm_use_neon = true
+# We have lib issues if we enable thumb
+arm_use_thumb = false
+arm_optionally_use_neon = false
+arm_version = 7
+arm_arch = "armv7-a"
+
+enable_widevine = true
+
+# Dawn seems to really want Vulkan - Chrome doesn't like our Vulkan
+# this causes a hiccup about a min after startup.
+# Disabling Dawn seems to create no penalty but this probably won't
+# be true indefinitely
+use_dawn = false
+skia_use_dawn = false
+
+# Enables screen sharing in hangouts
+enable_hangout_services_extension = true
+
+# We don't have the required GL extensions to enable GL passthrough so must
+# enable the validating decode otherwise we get a fatal GPU error
+enable_validating_command_decoder = true
+
+ffmpeg_branding = "Chrome"
+proprietary_codecs = true
+
+disable_fieldtrial_testing_config = true
+enable_nacl = false
+blink_symbol_level = 0
+symbol_level = 1
+is_official_build = true
+is_component_ffmpeg = true
+
--- /dev/null
+++ b/src/pi-util/dodiff.py
@@ -0,0 +1,35 @@
+#!/usr/bin/env python3
+
+import os, sys, string, subprocess
+
+# Local
+import gitscan, pipaths
+
+def doscan(outfile = sys.stdout):
+    revdict = gitscan.revdict()
+
+    cpath = gitscan.basepath()
+
+    for p in pipaths.pipaths:
+        os.chdir(os.path.join(cpath, p))
+        diff = subprocess.check_output(["git", "diff", "--ignore-submodules", revdict[p]], text=True)
+
+        header = False
+        lines = diff.split("\n")
+        # Remove terminal blank line
+        if lines[-1] == "":
+            lines.pop()
+        for line in lines:
+            if line.startswith("diff --git "):
+                header = True
+            if header:
+                line = line.replace(" a/", " a/" + p + "/")
+                line = line.replace(" b/", " b/" + p + "/")
+            if line.startswith("+++ "):
+                header = False
+            print(line, file=outfile)
+
+
+if __name__ == '__main__':
+    doscan()
+
--- /dev/null
+++ b/src/pi-util/gitscan.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python3
+
+import os, string, subprocess, sys
+
+# Local
+import pipaths
+
+def revdict():
+    revdict = {'src':pipaths.src_commit}
+    stuff = subprocess.check_output(["gclient", "revinfo"], text=True)
+    for line in stuff.split("\n"):
+        pathn = line.find(":")
+        commitn = line.rfind("@")
+        if pathn != -1 and commitn != -1 :
+             revdict[line[:pathn]] = line[commitn+1:]
+    return revdict
+
+def basepath():
+    cpath = os.getcwd()
+    if not cpath.endswith("/src"):
+        raise "CWD doesn't end with /src"
+
+    return cpath[:-4]
+
+def gitscan(args, nosrc = False, quiet=False):
+    rv = 0
+
+    oldcwd = os.getcwd()
+    rdict = revdict()
+    cpath = basepath()
+
+    for p in pipaths.pipaths:
+        if nosrc and p == "src":
+            continue
+
+        os.chdir(os.path.join(cpath, p))
+
+        gitargs = [a.replace("{PATH}", p).replace("{BASE}", rdict[p]) for a in args]
+        gitargs[0:0] = ["git"]
+
+        if not quiet:
+            print(">>>", p)
+
+        rv = subprocess.call(gitargs)
+        if rv != 0:
+            if not quiet:
+                print("Git returned non-zero error code", rv, "\ncwd =", os.getcwd(), "\ncmd =", gitargs)
+            break
+
+    os.chdir(oldcwd)
+    return rv
+
+
+if __name__ == '__main__':
+
+    if len(sys.argv) < 2:
+        print("Usage: gitscan [--gitscan-no-src] <git cmd>")
+        print("  substitutes {PATH} and {BASE}")
+        exit(0)
+
+    nosrc = False
+
+    if sys.argv[1] == "--gitscan-no-src":
+        nosrc = True
+        del sys.argv[1]
+
+    gitscan(sys.argv[1:], nosrc)
+
+
--- /dev/null
+++ b/src/pi-util/gngen.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+
+import os, ast, fileinput, subprocess, sys
+
+def docopy(name, vars, is_debug=False, is_ozone=False):
+    dir_suffix = ""
+    deb_str = "false"
+
+    if is_ozone:
+        ozone_str = "true"
+        dir_suffix = dir_suffix + "-ozone"
+    else:
+        ozone_str = "false"
+
+    if is_debug:
+        deb_str = "true"
+        dir_suffix = dir_suffix + "-deb"
+    else:
+        deb_str = "false"
+        dir_suffix = dir_suffix + "-rel"
+
+
+    dest_dir = os.path.join("out", name + dir_suffix)
+    src_file = os.path.join("pi-util", "defargs_" + name + ".gn")
+
+    # Ignore any errors making dir (in particular it already exists)
+    try:
+        os.makedirs(dest_dir)
+    except:
+        pass
+
+    dargs = open(os.path.join(dest_dir, "args.gn"), "wt")
+    dargs.write('# -- copied from: ' + src_file + '\n')
+
+    for line in fileinput.input(src_file):
+        dargs.write(line)
+
+    dargs.write('# -- created by ' + sys.argv[0] + '\n')
+    dargs.write('is_debug = ' + deb_str + '\n')
+    dargs.write('use_ozone = ' + ozone_str + '\n')
+    if is_ozone:
+        dargs.write('ozone_platform_x11 = true\n')
+        dargs.write('use_v4l2_codec = true\n')
+        dargs.write('use_v4l2_codec_rpi = true\n')
+
+    dargs.write('target_sysroot = "' + vars["target_sysroot"] + '"\n')
+    dargs.write('google_api_key = "' + vars["google_api_key"] + '"\n')
+    dargs.write('google_default_client_id = "' + vars["google_default_client_id"] + '"\n')
+    dargs.write('google_default_client_secret = "' + vars["google_default_client_secret"] + '"\n')
+
+    dargs.close()
+
+    subprocess.check_call(["gn", "gen", dest_dir])
+
+
+if __name__ == '__main__':
+    gyp_vars = {}
+    gypi = os.path.join(os.environ["HOME"], ".gyp", "include.gypi")
+    if os.path.isfile(gypi):
+        print("Importing from:", gypi)
+        gyps = open(gypi).read(-1)
+        gyp_vars = ast.literal_eval(gyps)["variables"]
+
+    gyp_vars["target_sysroot"] = os.path.abspath("build/linux/pios_bullseye_armhf-sysroot")
+
+    docopy("armv7-bullseye", gyp_vars, is_ozone=True)
+
+    gyp_vars["target_sysroot"] = os.path.abspath("build/linux/pios_bullseye_arm64-sysroot")
+
+    docopy("arm64-bullseye", gyp_vars, is_ozone=True)
+
--- /dev/null
+++ b/src/pi-util/makeall.sh
@@ -0,0 +1,49 @@
+set -e
+
+DISTRO=bullseye
+
+GET_BUILD_DEPS=
+if [ "$1" == "--build-deps" ]; then
+  GET_BUILD_DEPS=1
+  shift
+fi
+
+if [ "$1" == "" ]; then
+  echo "Usage: $0 [--build-deps] <git tag>"
+  exit 1
+fi
+
+echo === Check all committed
+pi-util/gitscan.py diff --name-status --exit-code
+echo === Reset third party libraries
+pi-util/gitscan.py --gitscan-no-src checkout {BASE}
+TAG=$1
+echo === Fetch chrome
+pi-util/gitscan.py fetch -t --all
+echo === Checkout chrome $TAG
+git checkout $TAG
+if [ ! $GET_BUILD_DEPS ]; then
+  echo === Skip system build dependancies
+else
+  echo === Get system build dependancies
+  build/install-build-deps.sh --arm
+fi
+echo === Sync third party libraries
+gclient sync -D --with_branch_heads
+# Shouldn't need this but the x bit often gets accidentally lost
+# due to core.filemode options
+chmod +x pi-util/*.sh pi-util/*.py
+echo === Checkout third party libraries
+pi-util/gitscan.py --gitscan-no-src checkout $TAG --
+echo === Get pi sysroots
+rm -rf out
+pi-util/rootgen.sh
+echo === Setup gn
+pi-util/gngen.py
+echo === Start build
+ninja -C out/armv7-${DISTRO}-ozone-rel chrome
+ninja -C out/arm64-${DISTRO}-ozone-rel chrome
+ninja -C out/armv6-${DISTRO}-ozone-rel chrome
+
+
+
--- /dev/null
+++ b/src/pi-util/patch2nd.awk
@@ -0,0 +1,3 @@
+/^\-\-\-/  { next }
+/^\+\+\+/ { $1="---"; print; $1="+++"; }
+{ print }
--- /dev/null
+++ b/src/pi-util/patchmake.sh
@@ -0,0 +1,50 @@
+set -e
+
+GET_BUILD_DEPS=
+if [ "$1" == "--build-deps" ]; then
+  GET_BUILD_DEPS=1
+  shift
+fi
+
+if [ "$1" == "" ]; then
+  echo "Usage: $0 [--build-deps] <patch file>"
+  exit 1
+fi
+if [ ! -e "$1" ]; then
+  echo "Didn't find patchfile $1"
+  exit 1
+fi
+
+PATCHFILE=$1
+TAG=${PATCHFILE##*/}
+TAG=${TAG:1}
+TAG=${TAG%%_*}
+
+echo "Version $TAG extracted from patchfile name"
+
+echo === Checkout chrome $TAG
+git checkout $TAG
+if [ ! $GET_BUILD_DEPS ]; then
+  echo === Skip system build dependancies
+else
+  echo === Get system build dependancies
+  build/install-build-deps.sh --unsupported --arm
+fi
+echo === Sync third party libraries
+gclient sync -D --with_branch_heads
+echo === Patch
+patch -p2 < $PATCHFILE
+# Shouldn't need this but the x bit often gets accidentally lost
+# due to core.filemode options
+chmod +x pi-util/*.sh pi-util/*.py
+echo === Get pi sysroots
+rm -rf out
+pi-util/rootgen.sh
+echo === Setup gn
+pi-util/gngen.py
+echo === Start build
+ninja -C out/armv7-bullseye-ozone-rel chrome
+ninja -C out/arm64-bullseye-ozone-rel chrome
+
+
+
--- /dev/null
+++ b/src/pi-util/pi-install-dev.sh
@@ -0,0 +1,53 @@
+# Install set to build appropriate root on a clean pi
+
+APT=aptitude
+#APT=apt-get
+
+sudo $APT install \
+comerr-dev \
+libasound2-dev \
+libatk1.0-dev \
+libatk-bridge2.0-dev \
+libcap-dev \
+libcups2-dev \
+libexif-dev \
+libffi-dev \
+libgbm-dev \
+libgconf2-dev \
+libgl1-mesa-dev \
+libgles-dev \
+libgtk-3-dev \
+libjpeg-dev \
+libkrb5-dev \
+libnspr4-dev \
+libnss3-dev \
+libpam0g-dev \
+libpango1.0-dev \
+libpci-dev \
+libpcre3-dev \
+libpipewire-0.2-dev \
+libssl-dev \
+libudev-dev \
+libx11-xcb-dev \
+libxcb1-dev \
+libxcb-dri3-dev \
+libxcb-shm0-dev \
+libxcb-image0-dev \
+libxss-dev \
+libxt-dev \
+libxtst-dev \
+mesa-common-dev \
+python-xcbgen \
+uuid-dev \
+xcb-proto
+
+echo Also need python-xcbgen on host
+
+# Pulse (hopefully) disabled
+# libpulse-dev \
+
+# Obviously replace paths appropriately below
+# Now run pi-util/syncroot.sh on the compile m/c to grab the appropriate
+# bits of the root and fix up the paths.
+# e.g. ON COMPILE M/C in src dir:
+# pi-util/syncroot.sh my-pi: raspian_jessie_pi1
--- /dev/null
+++ b/src/pi-util/pipaths.py
@@ -0,0 +1,7 @@
+pipaths=[
+    "src",
+    "src/third_party/libyuv",
+    "src/third_party/skia"]
+
+# Our base tag or commit no
+src_commit="125.0.6422.133"
--- /dev/null
+++ b/src/pi-util/pipewire_utils_h.patch
@@ -0,0 +1,11 @@
+--- a/build/linux/raspian_stretch_pi1-sysroot/usr/include/pipewire/utils.h
++++ b/build/linux/raspian_stretch_pi1-sysroot/usr/include/pipewire/utils.h
+@@ -52,7 +52,7 @@ static inline struct spa_pod *
+ pw_spa_pod_copy(const struct spa_pod *pod)
+ {
+ 	size_t size;
+-	struct spa_pod *c;
++	void *c;
+ 
+ 	if (pod == NULL)
+ 		return NULL;
--- /dev/null
+++ b/src/pi-util/rebase_liblinks.py
@@ -0,0 +1,37 @@
+#!/usr/bin/env python
+
+import os, sys
+from stat import *
+
+def walktree(top, callback, n, prefix):
+    '''recursively descend the directory tree rooted at top,
+       calling the callback function for each regular file'''
+
+    for f in os.listdir(top):
+        pathname = os.path.join(top, f)
+        mode = os.lstat(pathname).st_mode
+        if S_ISDIR(mode):
+            # It's a directory, recurse into it
+            walktree(pathname, callback, n+1, prefix)
+        elif S_ISLNK(mode):
+            # It's a file, call the callback function
+            callback(pathname, os.readlink(pathname), n, prefix)
+
+def visitfile(file, linkname, n, prefix):
+    if (linkname.startswith(prefix + 'lib/')):
+        newlink = "../" * n + linkname[len(prefix):]
+        print 'relinking', file, "->", newlink
+        os.remove(file)
+        os.symlink(newlink, file)
+
+if __name__ == '__main__':
+    argc = len(sys.argv)
+    if argc == 2:
+        walktree(sys.argv[1], visitfile, 0, "/")
+    elif argc == 3:
+        walktree(sys.argv[1], visitfile, 0, sys.argv[2])
+    else:
+        print "rebase_liblinks.py <local root> [<old sysroot>]"
+
+
+
--- /dev/null
+++ b/src/pi-util/rootgen.sh
@@ -0,0 +1,73 @@
+#!/bin/bash -e
+
+SRC_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && cd .. && pwd )"
+SCRIPT_DIR=$SRC_DIR/build/linux/sysroot_scripts
+BUILD_DIR=$SRC_DIR/out/sysroot-build/bullseye
+
+NOGEN=
+CLEAN=
+WANTXX=
+WANT32=
+WANT64=
+while [ "$1" != "" ] ; do
+  case $1 in
+    --nogen)
+      NOGEN=1
+      ;;
+    --clean)
+      CLEAN=1
+      ;;
+    --32)
+      WANT32=1
+      WANTXX=1
+      ;;
+    --64)
+      WANT64=1
+      WANTXX=1
+      ;;
+    *)
+      echo "Usage: $0 [--nogen][--clean][--32][--64]"
+      echo "  --nogen    Do not do sysroot generation"
+      echo "  --clean    Wipe existing sysroot caches"
+      echo "  --32       Only build 32-bit sysroot"
+      echo "  --64       Only build 64-bit sysroot"
+      exit 1
+      ;;
+  esac
+  shift
+done
+
+if [ ! $WANTXX ]; then
+  WANT32=1
+  WANT64=1
+fi
+
+if [ $CLEAN ]; then
+  rm -rf $SRC_DIR/build/linux/pios_*-sysroot
+  rm -f $SCRIPT_DIR/pios_keyring.gpg
+  rm -f $SCRIPT_DIR/generated_package_lists/pios-*
+  rm -rf $SRC_DIR/out/sysroot-build
+fi
+
+if [ ! $NOGEN ]; then
+  mkdir -p $BUILD_DIR
+  cd $SCRIPT_DIR
+  if [ ! -e pios_keyring.gpg ]; then
+    ./generate_keyring.sh --pios
+  fi
+  if [ $WANT32 ]; then
+    cd $SCRIPT_DIR
+    ./sysroot_creator.py --pios build armhf |& tee $BUILD_DIR/build-armhf.log
+    mkdir -pv ../pios_bullseye_armhf-sysroot
+    cd ../pios_bullseye_armhf-sysroot
+    bsdtar xf $BUILD_DIR/debian_pios-bullseye_armhf_sysroot.tar.xz
+  fi
+  if [ $WANT64 ]; then
+    cd $SCRIPT_DIR
+    ./sysroot_creator.py --pios build arm64 |& tee $BUILD_DIR/build-arm64.log
+    mkdir -pv ../pios_bullseye_arm64-sysroot
+    cd ../pios_bullseye_arm64-sysroot
+    bsdtar xf $BUILD_DIR/debian_pios-bullseye_arm64_sysroot.tar.xz
+  fi
+fi
+
--- /dev/null
+++ b/src/pi-util/settag.py
@@ -0,0 +1,63 @@
+#!/usr/bin/env python3
+
+import sys, os, subprocess
+
+# Local
+import pipaths
+import gitscan
+import dodiff
+import argparse
+
+def set_version(verstr):
+    pathname = "components/version_info/pi_patch_version_values.h"
+
+    with open(pathname, "wt") as f:
+        f.write("// Pi patch version - generated by pi-util/settag.py\n")
+        f.write('#define PI_PATCH_VERSION_STRING "' + verstr + '"\n')
+
+    subprocess.check_call(["git", "add", pathname])
+    subprocess.check_call(["git", "commit", "-m", "Update pi patch version to " + verstr])
+
+
+def set_tag(verstr):
+    newtag = "pi/" + pipaths.src_commit + "/" + verstr
+    print("Setting tag: " + newtag)
+    if gitscan.gitscan(["tag", newtag], quiet=True) != 0:
+        print("Tagging failed")
+        sys.exit(1)
+
+def set_tag_and_version(verstr):
+    set_version(verstr)
+    set_tag(verstr)
+
+if __name__ == '__main__':
+    argp = argparse.ArgumentParser(
+        description="Sets version info in pi_patch_version_values & tags source tree with it")
+    argp.add_argument("-p", action='store_true', help="Generate patch file")
+    argp.add_argument("-f", action='store_true', help="Overwrite existing patch file (only relevent if -p)")
+    argp.add_argument("-n", action='store_true', help="Do not tag")
+    argp.add_argument("verstr", help="Pi patch version string")
+    args = argp.parse_args()
+
+    patchpath = os.path.join("..", "v" + pipaths.src_commit + "_" + args.verstr + ".patch")
+
+    if args.p and not args.f and os.path.exists(patchpath):
+        print("Patchfile", patchpath, "already exists")
+        sys.exit(1)
+
+    if not args.n:
+        print("-- Checking all committed")
+        if gitscan.gitscan(["diff", "--ignore-submodules", "--name-status", "--exit-code"], quiet=True) != 0:
+            print("Status check failed - commit everything and try again")
+            sys.exit(1)
+
+        print("-- Generating & committing pi_patch_version_values.h")
+        set_version(args.verstr)
+        print("-- Generating tags")
+        set_tag(args.verstr)
+
+    if args.p:
+        print("-- Generating patch file: ", patchpath)
+        with open(patchpath, "wt") as f:
+            dodiff.doscan(f)
+
--- /dev/null
+++ b/src/pi-util/syncroot.sh
@@ -0,0 +1,70 @@
+set -e
+
+NEEDSVC=1
+TYPE=arm
+API=arm-linux-gnueabihf
+SYSROOT_DEFAULT=pios_buster_arm
+
+if [ "$1" == "--arm64" ]; then
+  shift
+  TYPE=arm64
+  NEEDSVC=
+  API=aarch64-linux-gnu
+  SYSROOT_DEFAULT=pios_buster_arm64
+fi
+
+if [ "$1" == "" ]; then
+  echo Usage: $0 [--arm64] \<src_dir\> [\<rootname\>]
+  echo src_dir is a source for rsync so may contain m/c name.
+  echo rootname will be set to \"pios_buster_arm\" if missing
+  echo e.g.: pi-util/syncroot.sh my-pi:
+  exit 1
+fi
+
+SYSROOT_NAME=$2
+if [ "$SYSROOT_NAME" == "" ]; then
+  SYSROOT_NAME=$SYSROOT_DEFAULT
+fi
+
+DST_ROOT=`gclient root`
+DST=$DST_ROOT/src/build/linux/$SYSROOT_NAME-sysroot
+SRC=$1
+
+if [ ! -d $DST_ROOT/src/build/linux ]; then
+  echo We don\'t appear to be in a Chrome build tree
+  exit 1
+fi
+
+echo Copying root for $TYPE
+echo Sync src:  $SRC
+echo Sync dest: $DST
+
+mkdir -p $DST/lib
+mkdir -p $DST/opt/vc/include
+mkdir -p $DST/usr/lib/pkgconfig
+mkdir -p $DST/usr/bin
+mkdir -p $DST/usr/share
+
+rsync -rl $SRC/lib $DST
+if [ $NEEDSVC ]; then
+  #### MUST NOT include /opt/vc/include/*GL*
+  # Creates conflicts with GL includes inside Chrome
+  rsync -rl $SRC/opt/vc/lib $DST/opt/vc
+  rsync -rl $SRC/opt/vc/include/interface $DST/opt/vc/include
+fi
+rsync -rl --exclude cups/backend $SRC/usr/lib $DST/usr
+rsync -rl $SRC/usr/include $DST/usr
+rsync -rl $SRC/usr/share/pkgconfig $DST/usr/share
+rsync -rl $SRC/usr/share/xcb $DST/usr/share
+rsync -rl $SRC/usr/bin/cups-config $DST/usr/bin
+
+# Fix up pipewire issue
+if [ -e $DST/usr/include/pipewire/utils.h ]; then
+  sed 's/struct spa_pod \*c/void \* c/' < $DST/usr/include/pipewire/utils.h > u.h
+  mv u.h $DST/usr/include/pipewire/utils.h
+fi
+
+cd $DST/usr/lib/pkgconfig
+ln -sf ../$API/pkgconfig/* .
+cd ../../../../../..
+pi-util/rebase_liblinks.py $DST
--- a/src/third_party/widevine/cdm/widevine.gni
+++ b/src/third_party/widevine/cdm/widevine.gni
@@ -27,7 +27,7 @@ if (is_chromeos && !is_chromeos_device)
 library_widevine_cdm_available =
     (is_chromeos &&
      (target_cpu == "x64" || target_cpu == "arm" || target_cpu == "arm64")) ||
-    (target_os == "linux" && target_cpu == "x64") ||
+    (target_os == "linux" && (target_cpu == "x64" || target_cpu == "arm")) ||
     (target_os == "mac" && (target_cpu == "x64" || target_cpu == "arm64")) ||
     (target_os == "win" &&
      (target_cpu == "x86" || target_cpu == "x64" || target_cpu == "arm64"))
--- a/src/ui/gl/init/gl_factory.cc
+++ b/src/ui/gl/init/gl_factory.cc
@@ -103,6 +103,15 @@ GLImplementationParts GetRequestedGLImpl
   std::optional<GLImplementationParts> impl_from_cmdline =
       GetRequestedGLImplementationFromCommandLine(cmd, fallback_to_software_gl);
 
+  // RPI: We always want EGL as our preferred render
+  // Easiest to simply add here - angle + GLES +/- EGL crashes
+  // so add here after the angle add.
+  //
+  // BEWARE: falling back from EGL to anything else with --in-process-gpu
+  // set produces a segfault when we exit so ideally we would only do this
+  // on Pi4/(F)KMS?
+  allowed_impls.insert(allowed_impls.begin(), GLImplementationParts(kGLImplementationEGLGLES2));
+
   // The default implementation is always the first one in list.
   if (!impl_from_cmdline)
     return allowed_impls[0];
--- a/src/ui/linux/display_server_utils.cc
+++ b/src/ui/linux/display_server_utils.cc
@@ -121,6 +121,13 @@ void SetOzonePlatformForLinuxIfNeeded(ba
       command_line.AppendSwitchASCII(switches::kOzonePlatform,
                                      MaybeFixPlatformName(ozone_platform_hint));
     }
+#if BUILDFLAG(IS_OZONE_WAYLAND)
+    // Pick wayland as hint if we have it
+    else {
+      command_line.AppendSwitchASCII(switches::kOzonePlatform,
+                                     MaybeFixPlatformName(kPlatformWayland));
+    }
+#endif
   }
 }
 
--- a/src/ui/ozone/platform/wayland/host/wayland_connection.h
+++ b/src/ui/ozone/platform/wayland/host/wayland_connection.h
@@ -355,8 +355,11 @@ class WaylandConnection {
   void DumpState(std::ostream& out) const;
 
   bool UseImplicitSyncInterop() const {
-    return !linux_explicit_synchronization_v1() &&
-           WaylandBufferManagerHost::SupportsImplicitSyncInterop();
+    // *** Broken, at least on Pi (last checked v125)
+    //  Causes flickering / out-of-order / old frames when navigating or animating
+    return false;
+//    return !linux_explicit_synchronization_v1() &&
+//           WaylandBufferManagerHost::SupportsImplicitSyncInterop();
   }
 
  private:
--- a/src/third_party/libyuv/BUILD.gn
+++ b/src/third_party/libyuv/BUILD.gn
@@ -188,7 +188,6 @@ static_library("libyuv_internal") {
     configs += [ "//build/config/compiler:optimize_max" ]
   }
 
-  # To enable AVX2 or other cpu optimization, pass flag here
   if (!is_win) {
     cflags = [
       # "-mpopcnt",
@@ -197,6 +196,11 @@ static_library("libyuv_internal") {
       "-ffp-contract=fast",  # Enable fma vectorization for NEON.
     ]
   }
+
+  if (current_cpu == "arm" && arm_version < 7) {
+    # This emables the neon test even if the current compile doesn't support it
+    defines += [ "LIBYUV_NEON" ]
+  }
 }
 
 if (libyuv_use_neon) {
@@ -229,6 +233,11 @@ if (libyuv_use_neon) {
 
     if (current_cpu != "arm64") {
       configs -= [ "//build/config/compiler:compiler_arm_fpu" ]
+      if (arm_version < 7) {
+        configs += [
+          "//build/config/compiler:force_march_armv7",
+        ]
+      }
       cflags = [ "-mfpu=neon" ]
     }
   }
--- a/src/third_party/libyuv/source/cpu_id.cc
+++ b/src/third_party/libyuv/source/cpu_id.cc
@@ -132,6 +132,18 @@ static int GetXCR0() {
 #pragma optimize("g", on)
 #endif
 
+#ifdef __ARMEL__
+// This is (a) simpler and (b) works in sandbox vs the /proc/cpuinfo method
+#include <sys/auxv.h>
+
+int ArmCpuCaps(const char* cpuinfo_name) {
+  const unsigned long auxval = getauxval(AT_HWCAP);
+
+  // Documentation suggests that getauxval(AT_HWCAP) should return a pointer
+  // to a bit array, but evidence suggests it returns a simple bit field
+  return ((auxval & HWCAP_ARM_NEON) != 0 ? kCpuHasNEON : 0);
+}
+#else
 // Based on libvpx arm_cpudetect.c
 // For Arm, but public to allow testing on any CPU
 LIBYUV_API SAFEBUFFERS int ArmCpuCaps(const char* cpuinfo_name) {
@@ -161,6 +173,7 @@ LIBYUV_API SAFEBUFFERS int ArmCpuCaps(co
   fclose(f);
   return 0;
 }
+#endif
 
 LIBYUV_API SAFEBUFFERS int RiscvCpuCaps(const char* cpuinfo_name) {
   char cpuinfo_line[512];
--- a/src/third_party/skia/src/core/SkBitmapProcState.cpp
+++ b/src/third_party/skia/src/core/SkBitmapProcState.cpp
@@ -24,6 +24,7 @@
 class SkImage;
 class SkImage_Base;
 
+#if !defined(SK_ARM_HAS_NEON) || defined(__ARM_64BIT_STATE)
 // One-stop-shop shader for,
 //   - nearest-neighbor sampling (_nofilter_),
 //   - clamp tiling in X and Y both (Clamp_),
@@ -79,6 +80,199 @@ static void Clamp_S32_opaque_D32_nofilte
         }
     }
 }
+#endif
+
+// We define two variants of this: one for 32-bit ARM NEON, and one generic C:
+
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+static inline void Clamp_S32_opaque_D32_nofilter_DX_shaderproc_core_neon(SkPMColor* __restrict__ &dst, const SkPMColor* __restrict__ src, int core, SkFractionalInt fx, const SkFractionalInt dx)
+{
+    const SkPMColor*p = src + (int32_t)(fx >> 32);
+    uint32_t accum = (uint32_t) fx;
+    const SkPMColor *p2;
+    __asm__ volatile (
+            "cmp     %[core], #0           \n\t"
+            "it      ne                    \n\t"
+            "tstne   %[dst], #0xc          \n\t"
+            "beq     2f                    \n\t"
+            "1:                            \n\t"
+            "vldr    s0, [%[p]]            \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p], %[inc1]         \n\t"
+            "addcs   %[p], %[inc2]         \n\t"
+            "vstm    %[dst]!, {s0}         \n\t"
+            "subs    %[core], #1           \n\t"
+            "it      ne                    \n\t"
+            "tstne   %[dst], #0xc          \n\t"
+            "bne     1b                    \n\t"
+            "2:                            \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p2], %[p], %[inc1]  \n\t"
+            "addcs   %[p2], %[p], %[inc2]  \n\t"
+            "subs    %[core], #4           \n\t"
+            "bcc     4f                    \n\t"
+            "3:                            \n\t"
+            "vldr    s0, [%[p]]            \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p], %[p2], %[inc1]  \n\t"
+            "addcs   %[p], %[p2], %[inc2]  \n\t"
+            "vldr    s1, [%[p2]]           \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p2], %[p], %[inc1]  \n\t"
+            "addcs   %[p2], %[p], %[inc2]  \n\t"
+            "vldr    s2, [%[p]]            \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p], %[p2], %[inc1]  \n\t"
+            "addcs   %[p], %[p2], %[inc2]  \n\t"
+            "vldr    s3, [%[p2]]           \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p2], %[p], %[inc1]  \n\t"
+            "addcs   %[p2], %[p], %[inc2]  \n\t"
+            "vst1.32 {q0}, [%[dst] :128]!  \n\t"
+            "subs    %[core], #4           \n\t"
+            "bcs     3b                    \n\t"
+            "4:                            \n\t"
+            "adds    %[core], #4           \n\t"
+            "beq     6f                    \n\t"
+            "5:                            \n\t"
+            "vldr    s0, [%[p]]            \n\t"
+            "mov     %[p], %[p2]           \n\t"
+            "adds    %[accum], %[dx]       \n\t"
+            "ite     cc                    \n\t"
+            "addcc   %[p2], %[p], %[inc1]  \n\t"
+            "addcs   %[p2], %[p], %[inc2]  \n\t"
+            "vstm    %[dst]!, {s0}         \n\t"
+            "subs    %[core], #1           \n\t"
+            "bne     5b                    \n\t"
+            "6:                            \n\t"
+    : // Outputs
+            [accum]"+r"(accum),
+             [core]"+r"(core),
+              [dst]"+r"(dst),
+                [p]"+r"(p),
+               [p2]"=&r"(p2)
+    : // Inputs
+              [dx]"r"((int32_t) dx),
+            [inc1]"r"((int32_t)(dx >> 32) * 4),
+            [inc2]"r"(((int32_t)(dx >> 32) + 1) * 4)
+    : // Clobbers
+            "cc", "memory"
+    );
+}
+#endif
+
+#if 0
+static inline void Clamp_S32_opaque_D32_nofilter_DX_shaderproc_core(SkPMColor* __restrict__ &dst, const SkPMColor* __restrict__ src, int core, SkFractionalInt fx, const SkFractionalInt dx)
+{
+    const SkPMColor*p = src + (int32_t)(fx >> 32);
+    uint32_t accum = (uint32_t) fx;
+    for (; core > 0; --core) {
+        *dst++ = *p;
+        uint32_t prev_accum = accum;
+        accum += (int32_t) dx;
+        if (accum < prev_accum) /* i.e. carry set */
+            p += (int32_t)(dx >> 32) + 1;
+        else
+            p += (int32_t)(dx >> 32);
+    }
+}
+#endif
+
+#define Clamp_S32_opaque_D32_nofilter_DX_shaderproc_template(SUFFIX)                               \
+static void Clamp_S32_opaque_D32_nofilter_DX_shaderproc(const void* sIn, int x, int y,             \
+                                                        SkPMColor* SK_RESTRICT dst,  int count) {  \
+    const SkBitmapProcState& s = *static_cast<const SkBitmapProcState*>(sIn);                      \
+    SkASSERT(s.fAlphaScale == 256);                                                                \
+                                                                                                   \
+    const unsigned maxX = s.fPixmap.width() - 1;                                                   \
+    SkFractionalInt fx;                                                                            \
+    int dstY;                                                                                      \
+    {                                                                                              \
+        const SkBitmapProcStateAutoMapper mapper(s, x, y);                                         \
+        const unsigned maxY = s.fPixmap.height() - 1;                                              \
+        dstY = SkTPin(mapper.intY(), 0, (int)maxY);                                                    \
+        fx = mapper.fractionalIntX();                                                              \
+    }                                                                                              \
+                                                                                                   \
+    const SkPMColor* SK_RESTRICT src = s.fPixmap.addr32(0, dstY);                                  \
+    const SkFractionalInt dx = s.fInvSxFractionalInt;                                              \
+                                                                                                   \
+    int core;                                                                                      \
+                                                                                                   \
+    /* The unscaled case is easily common enough to be worth special-casing.                       \
+     * The system memcpy() is typically already heavily optimized, so just use that.               \
+     */                                                                                            \
+    if (dx == 0x100000000ll) {                                                                     \
+        int32_t fx_integer = fx >> 32;                                                             \
+        if (fx_integer < 0) {                                                                      \
+            int left = std::min(-fx_integer, count);                                                \
+            fx_integer += left;                                                                    \
+            count -= left;                                                                         \
+            for (; left > 0; --left)                                                               \
+                *dst++ = src[0];                                                                   \
+        }                                                                                          \
+        if (fx_integer < (int)maxX) {                                                              \
+            core = std::min((int)maxX + 1 - fx_integer, count);                                     \
+            memcpy(dst, src + fx_integer, core * sizeof (uint32_t));                               \
+            dst += core;                                                                           \
+            count -= core;                                                                         \
+        }                                                                                          \
+        for (; count > 0; --count) {                                                               \
+            *dst++ = src[maxX];                                                                    \
+        }                                                                                          \
+    }                                                                                              \
+                                                                                                   \
+    /* Handle other non-reflected scale factors. */                                                \
+    else if (dx >= 0) {                                                                            \
+        for (; fx < 0 && count > 0; --count) {                                                     \
+            *dst++ = src[0];                                                                       \
+            fx += dx;                                                                              \
+        }                                                                                          \
+        if ((int32_t)(fx >> 32) > (int)maxX)                                                       \
+            core = 0;                                                                              \
+        else if ((int32_t)((fx + (count - 1) * dx) >> 32) <= (int)maxX)                            \
+            core = count;                                                                          \
+        else                                                                                       \
+            core = (int32_t)(((((SkFractionalInt) maxX) << 32) + 0xffffffff - fx) / dx) + 1;       \
+        Clamp_S32_opaque_D32_nofilter_DX_shaderproc_core##SUFFIX(dst, src, core, fx, dx);          \
+        count -= core;                                                                             \
+        for (; count > 0; --count) {                                                               \
+            *dst++ = src[maxX];                                                                    \
+        }                                                                                          \
+    }                                                                                              \
+                                                                                                   \
+    /* It's not clear if reflection is used, but it's a relatively                                 \
+     * simple variation on the non-reflected case. */                                              \
+    else                                                                                           \
+    {                                                                                              \
+        for (; (int32_t)(fx >> 32) > (int)maxX && count > 0; --count) {                            \
+            *dst++ = src[maxX];                                                                    \
+            fx += dx;                                                                              \
+        }                                                                                          \
+        if (fx < 0)                                                                                \
+            core = 0;                                                                              \
+        else if (fx + (count - 1) * dx >= 0)                                                       \
+            core = count;                                                                          \
+        else                                                                                       \
+            core = (int32_t)(fx / -dx) + 1;                                                        \
+        Clamp_S32_opaque_D32_nofilter_DX_shaderproc_core##SUFFIX(dst, src, core, fx, dx);          \
+        count -= core;                                                                             \
+        for (; count > 0; --count) {                                                               \
+            *dst++ = src[0];                                                                       \
+        }                                                                                          \
+    }                                                                                              \
+}
+
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+Clamp_S32_opaque_D32_nofilter_DX_shaderproc_template(_neon)
+#endif
+
 
 static void S32_alpha_D32_nofilter_DX(const SkBitmapProcState& s,
                                       const uint32_t* xy, int count, SkPMColor* colors) {
@@ -255,7 +449,11 @@ bool SkBitmapProcState::chooseProcs() {
     fMatrixProc = this->chooseMatrixProc(translate_only);
     SkASSERT(fMatrixProc);
 
-    fSampleProc32 = fBilerp ? SkOpts::S32_alpha_D32_filter_DX : S32_alpha_D32_nofilter_DX;
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+    fSampleProc32 = fBilerp ? (fAlphaScale == 256 ? SkOpts::S32_opaque_D32_filter_DX : SkOpts::S32_alpha_D32_filter_DX) : S32_alpha_D32_nofilter_DX  ;
+#else
+    fSampleProc32 = fBilerp ? SkOpts::S32_alpha_D32_filter_DX   : S32_alpha_D32_nofilter_DX  ;
+#endif
     SkASSERT(fSampleProc32);
 
     // our special-case shaderprocs
--- a/src/third_party/skia/src/core/SkBitmapProcState.h
+++ b/src/third_party/skia/src/core/SkBitmapProcState.h
@@ -216,7 +216,10 @@ namespace SkOpts {
     // SkBitmapProcState optimized Shader, Sample, or Matrix procs.
     extern void (*S32_alpha_D32_filter_DX)(const SkBitmapProcState&,
                                            const uint32_t* xy, int count, SkPMColor*);
-
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+    extern void (*S32_opaque_D32_filter_DX)(const SkBitmapProcState&,
+                                           const uint32_t* xy, int count, SkPMColor*);
+#endif
     void Init_BitmapProcState();
 }  // namespace SkOpts
 
--- a/src/third_party/skia/src/core/SkBitmapProcState_matrixProcs.cpp
+++ b/src/third_party/skia/src/core/SkBitmapProcState_matrixProcs.cpp
@@ -272,6 +272,530 @@ static unsigned clamp(SkFixed fx, int ma
     return SkTPin(fx >> 16, 0, max);
 }
 
+// Clamp/Clamp and Repeat/Repeat have NEON or portable implementations.
+#if defined(SK_ARM_HAS_NEON)
+    #include <arm_neon.h>
+
+    // TODO: this is a fine drop-in for decal_nofilter_scale() generally.
+    static void decal_nofilter_scale_neon(uint32_t dst[], SkFixed fx, SkFixed dx, int count) {
+        if (count >= 8) {
+            // SkFixed is 16.16 fixed point
+            SkFixed dx8 = dx * 8;
+            int32x4_t vdx8 = vdupq_n_s32(dx8);
+
+            // setup lbase and hbase
+            int32x4_t lbase, hbase;
+            lbase = vdupq_n_s32(fx);
+            lbase = vsetq_lane_s32(fx + dx, lbase, 1);
+            lbase = vsetq_lane_s32(fx + dx + dx, lbase, 2);
+            lbase = vsetq_lane_s32(fx + dx + dx + dx, lbase, 3);
+            hbase = lbase + vdupq_n_s32(4 * dx);
+
+            do {
+                // store the upper 16 bits
+                vst1q_u32(dst, vreinterpretq_u32_s16(
+                    vuzpq_s16(vreinterpretq_s16_s32(lbase), vreinterpretq_s16_s32(hbase)).val[1]
+                ));
+
+                // on to the next group of 8
+                lbase += vdx8;
+                hbase += vdx8;
+                dst += 4; // we did 8 elements but the result is twice smaller
+                count -= 8;
+                fx += dx8;
+            } while (count >= 8);
+        }
+
+        uint16_t* xx = (uint16_t*)dst;
+        for (int i = count; i > 0; --i) {
+            *xx++ = SkToU16(fx >> 16); fx += dx;
+        }
+    }
+
+    static void decal_filter_scale_neon(uint32_t dst[], SkFixed fx, SkFixed dx, int count) {
+#ifndef __ARM_64BIT_STATE
+        SkASSERT(((fx + (count-1) * dx) >> (16 + 14)) == 0);
+        fx = (fx << 2) + 1;
+        dx <<= 2;
+        while (((uintptr_t) dst & 0xf) && --count >= 0) {
+            *dst++ = (fx & 0xffffc001) + (fx >> 18);
+            fx += dx;
+        }
+        if ((count -= 4) >= 0) {
+            uint32_t tmp;
+            __asm__ (
+                    "adr         %[tmp], 1f                  \n\t"
+                    "vmvn.i32    q10, #0x3fff                \n\t"
+                    "vld1.32     {q11}, [%[tmp]]             \n\t"
+                    "vdup.32     q8, %[fx]                   \n\t"
+                    "vdup.32     q9, %[dx]                   \n\t"
+                    "vsra.u32    q10, #31                    \n\t"
+                    "vmla.u32    q8, q9, q11                 \n\t"
+                    "vshl.u32    q9, #2                      \n\t"
+                    "b           2f                          \n\t"
+                    "1:                                      \n\t"
+                    ".long       0                           \n\t"
+                    ".long       1                           \n\t"
+                    ".long       2                           \n\t"
+                    ".long       3                           \n\t"
+                    "2:                                      \n\t"
+                    "vand        q11, q8, q10                \n\t"
+                    "vshr.u32    q12, q8, #18                \n\t"
+                    "vadd.i32    q11, q12                    \n\t"
+                    "vadd.i32    q8, q9                      \n\t"
+                    "subs        %[count], #4                \n\t"
+                    "vst1.32     {q11}, [%[dst]:128]!        \n\t"
+                    "bpl         2b                          \n\t"
+                    "vmov.32     %[fx], d16[0]               \n\t"
+            : // Outputs
+                    [count]"+l"(count),
+                      [dst]"+r"(dst),
+                       [fx]"+r"(fx),
+                      [tmp]"=&r"(tmp)
+            : // Inputs
+                    [dx]"r"(dx)
+            : // Clobbers
+                    "cc", "memory"
+            );
+        }
+        if ((count += 4-1) >= 0) {
+            do {
+                *dst++ = (fx & 0xffffc001) + (fx >> 18);
+                fx += dx;
+            } while (--count >= 0);
+        }
+#else // !defined(__ARM_64BIT_STATE)
+        if (count >= 8) {
+            SkFixed dx8 = dx * 8;
+            int32x4_t vdx8 = vdupq_n_s32(dx8);
+
+            int32x4_t wide_fx, wide_fx2;
+            wide_fx = vdupq_n_s32(fx);
+            wide_fx = vsetq_lane_s32(fx + dx, wide_fx, 1);
+            wide_fx = vsetq_lane_s32(fx + dx + dx, wide_fx, 2);
+            wide_fx = vsetq_lane_s32(fx + dx + dx + dx, wide_fx, 3);
+
+            wide_fx2 = vaddq_s32(wide_fx, vdupq_n_s32(4 * dx));
+
+            while (count >= 8) {
+                int32x4_t wide_out;
+                int32x4_t wide_out2;
+
+                wide_out = vshlq_n_s32(vshrq_n_s32(wide_fx, 12), 14);
+                wide_out = wide_out | (vshrq_n_s32(wide_fx,16) + vdupq_n_s32(1));
+
+                wide_out2 = vshlq_n_s32(vshrq_n_s32(wide_fx2, 12), 14);
+                wide_out2 = wide_out2 | (vshrq_n_s32(wide_fx2,16) + vdupq_n_s32(1));
+
+                vst1q_u32(dst, vreinterpretq_u32_s32(wide_out));
+                vst1q_u32(dst+4, vreinterpretq_u32_s32(wide_out2));
+
+                dst += 8;
+                fx += dx8;
+                wide_fx += vdx8;
+                wide_fx2 += vdx8;
+                count -= 8;
+            }
+        }
+
+        if (count & 1)
+        {
+            SkASSERT((fx >> (16 + 14)) == 0);
+            *dst++ = (fx >> 12 << 14) | ((fx >> 16) + 1);
+            fx += dx;
+        }
+        while ((count -= 2) >= 0)
+        {
+            SkASSERT((fx >> (16 + 14)) == 0);
+            *dst++ = (fx >> 12 << 14) | ((fx >> 16) + 1);
+            fx += dx;
+
+            *dst++ = (fx >> 12 << 14) | ((fx >> 16) + 1);
+            fx += dx;
+        }
+#endif
+    }
+
+    static inline int16x8_t clamp8(int32x4_t low, int32x4_t high, unsigned max) {
+        int16x8_t res;
+
+        // get the hi 16s of all those 32s
+        res = vuzpq_s16(vreinterpretq_s16_s32(low), vreinterpretq_s16_s32(high)).val[1];
+
+        // clamp
+        res = vmaxq_s16(res, vdupq_n_s16(0));
+        res = vminq_s16(res, vdupq_n_s16(max));
+
+        return res;
+    }
+
+    static inline int32x4_t clamp4(int32x4_t f, unsigned max) {
+        int32x4_t res;
+
+        // get the hi 16s of all those 32s
+        res = vshrq_n_s32(f, 16);
+
+        // clamp
+        res = vmaxq_s32(res, vdupq_n_s32(0));
+        res = vminq_s32(res, vdupq_n_s32(max));
+
+        return res;
+    }
+
+    static inline int32x4_t extract_low_bits_clamp4(int32x4_t fx, unsigned) {
+        int32x4_t ret;
+
+        ret = vshrq_n_s32(fx, 12);
+
+        /* We don't need the mask below because the caller will
+         * overwrite the non-masked bits
+         */
+        //ret = vandq_s32(ret, vdupq_n_s32(0xF));
+
+        return ret;
+    }
+
+    static inline int16x8_t repeat8(int32x4_t low, int32x4_t high, unsigned max) {
+        uint16x8_t res;
+        uint32x4_t tmpl, tmph;
+
+        // get the lower 16 bits
+        res = vuzpq_u16(vreinterpretq_u16_s32(low), vreinterpretq_u16_s32(high)).val[0];
+
+        // bare multiplication, not SkFixedMul
+        tmpl = vmull_u16(vget_low_u16(res), vdup_n_u16(max+1));
+        tmph = vmull_u16(vget_high_u16(res), vdup_n_u16(max+1));
+
+        // extraction of the 16 upper bits
+        res = vuzpq_u16(vreinterpretq_u16_u32(tmpl), vreinterpretq_u16_u32(tmph)).val[1];
+
+        return vreinterpretq_s16_u16(res);
+    }
+
+    static inline int32x4_t repeat4(int32x4_t f, unsigned max) {
+        uint16x4_t res;
+        uint32x4_t tmp;
+
+        // get the lower 16 bits
+        res = vmovn_u32(vreinterpretq_u32_s32(f));
+
+        // bare multiplication, not SkFixedMul
+        tmp = vmull_u16(res, vdup_n_u16(max+1));
+
+        // extraction of the 16 upper bits
+        tmp = vshrq_n_u32(tmp, 16);
+
+        return vreinterpretq_s32_u32(tmp);
+    }
+
+    static inline int32x4_t extract_low_bits_repeat_mirror4(int32x4_t fx, unsigned max) {
+        uint16x4_t res;
+        uint32x4_t tmp;
+        int32x4_t ret;
+
+        // get the lower 16 bits
+        res = vmovn_u32(vreinterpretq_u32_s32(fx));
+
+        // bare multiplication, not SkFixedMul
+        tmp = vmull_u16(res, vdup_n_u16(max + 1));
+
+        // shift and mask
+        ret = vshrq_n_s32(vreinterpretq_s32_u32(tmp), 12);
+
+        /* We don't need the mask below because the caller will
+         * overwrite the non-masked bits
+         */
+        //ret = vandq_s32(ret, vdupq_n_s32(0xF));
+
+        return ret;
+    }
+
+    template <unsigned   (*tile)(SkFixed, int),
+              int16x8_t (*tile8)(int32x4_t, int32x4_t, unsigned),
+             bool tryDecal>
+    static void nofilter_scale_neon(const SkBitmapProcState& s,
+                                    uint32_t xy[], int count, int x, int y) {
+
+        // we store y, x, x, x, x, x
+        const unsigned maxX = s.fPixmap.width() - 1;
+        SkFractionalInt fx;
+        {
+            const SkBitmapProcStateAutoMapper mapper(s, x, y);
+            const unsigned maxY = s.fPixmap.height() - 1;
+            *xy++ = tile(mapper.fixedY(), maxY);
+            fx = mapper.fractionalIntX();
+        }
+
+        if (0 == maxX) {
+            // all of the following X values must be 0
+            memset(xy, 0, count * sizeof(uint16_t));
+            return;
+        }
+
+        const SkFractionalInt dx = s.fInvSxFractionalInt;
+
+        // test if we don't need to apply the tile proc
+        const SkFixed fixedFx = SkFractionalIntToFixed(fx);
+        const SkFixed fixedDx = SkFractionalIntToFixed(dx);
+        if (tryDecal && can_truncate_to_fixed_for_decal(fixedFx, fixedDx, count, maxX)) {
+            decal_nofilter_scale_neon(xy, fixedFx, fixedDx, count);
+            return;
+        }
+
+        if (count >= 8) {
+            SkFractionalInt dx2 = dx+dx;
+            SkFractionalInt dx4 = dx2+dx2;
+            SkFractionalInt dx8 = dx4+dx4;
+
+            // now build fx/fx+dx/fx+2dx/fx+3dx
+            SkFractionalInt fx1, fx2, fx3;
+            int32x4_t lbase, hbase;
+            int16_t *dst16 = (int16_t *)xy;
+
+            fx1 = fx+dx;
+            fx2 = fx1+dx;
+            fx3 = fx2+dx;
+
+            lbase = vdupq_n_s32(SkFractionalIntToFixed(fx));
+            lbase = vsetq_lane_s32(SkFractionalIntToFixed(fx1), lbase, 1);
+            lbase = vsetq_lane_s32(SkFractionalIntToFixed(fx2), lbase, 2);
+            lbase = vsetq_lane_s32(SkFractionalIntToFixed(fx3), lbase, 3);
+            hbase = vaddq_s32(lbase, vdupq_n_s32(SkFractionalIntToFixed(dx4)));
+
+            // store & bump
+            while (count >= 8) {
+
+                int16x8_t fx8;
+
+                fx8 = tile8(lbase, hbase, maxX);
+
+                vst1q_s16(dst16, fx8);
+
+                // but preserving base & on to the next
+                lbase = vaddq_s32 (lbase, vdupq_n_s32(SkFractionalIntToFixed(dx8)));
+                hbase = vaddq_s32 (hbase, vdupq_n_s32(SkFractionalIntToFixed(dx8)));
+                dst16 += 8;
+                count -= 8;
+                fx += dx8;
+            }
+            xy = (uint32_t *) dst16;
+        }
+
+        uint16_t* xx = (uint16_t*)xy;
+        for (int i = count; i > 0; --i) {
+            *xx++ = tile(SkFractionalIntToFixed(fx), maxX);
+            fx += dx;
+        }
+    }
+
+    template <unsigned              (*tile )(SkFixed, int),
+              int32x4_t             (*tile4)(int32x4_t, unsigned),
+              unsigned  (*extract_low_bits )(SkFixed, int),
+              int32x4_t (*extract_low_bits4)(int32x4_t, unsigned),
+              bool tryDecal>
+    static void filter_scale_neon(const SkBitmapProcState& s,
+                                  unsigned int * xy, int count, int x, int y) {
+
+        auto pack = [&](SkFixed f, unsigned max, SkFixed one) {
+            unsigned i = tile(f, max);
+            i = (i << 4) | extract_low_bits(f, max);
+            return (i << 14) | (tile((f + one), max));
+        };
+
+        auto pack4 = [&](int32x4_t f, unsigned max, SkFixed one) {
+            int32x4_t ret, res;
+
+            res = tile4(f, max);
+
+            ret = extract_low_bits4(f, max);
+            ret = vsliq_n_s32(ret, res, 4);
+
+            res = tile4(f + vdupq_n_s32(one), max);
+            ret = vorrq_s32(vshlq_n_s32(ret, 14), res);
+
+            return ret;
+        };
+
+        const unsigned maxX = s.fPixmap.width() - 1;
+        const SkFixed one = s.fFilterOneX;
+        const SkFractionalInt dx = s.fInvSxFractionalInt;
+        SkFractionalInt fx;
+
+        {
+            const SkBitmapProcStateAutoMapper mapper(s, x, y);
+            const SkFixed fy = mapper.fixedY();
+            const unsigned maxY = s.fPixmap.height() - 1;
+            // compute our two Y values up front
+            *xy++ = pack(fy, maxY, s.fFilterOneY);
+            // now initialize fx
+            fx = mapper.fractionalIntX();
+        }
+
+        // test if we don't need to apply the tile proc
+        const SkFixed fixedFx = SkFractionalIntToFixed(fx);
+        const SkFixed fixedDx = SkFractionalIntToFixed(dx);
+        if (tryDecal && can_truncate_to_fixed_for_decal(fixedFx, fixedDx, count, maxX)) {
+            decal_filter_scale_neon(xy, fixedFx, fixedDx, count);
+            return;
+        }
+
+#ifndef __ARM_64BIT_STATE
+        if (tile == clamp && one == SK_Fixed1) {
+            SkASSERT(maxX < (1<<14)-1);
+            if (dx >= 0) {
+                --count;
+                while (count >= 0 && fx < 0) {
+                    *xy++ = 0;
+                    fx += dx;
+                    --count;
+                }
+                while (count >= 0 && ((uintptr_t) xy & 0xf) && fx < ((SkFractionalInt) maxX << 32)) {
+                    *xy++ = ((uint32_t)(fx >> 14) & 0xffffc000) + (uint32_t)(fx >> 32) + 1;
+                    fx += dx;
+                    --count;
+                }
+                if ((count -= 8-1) >= 0 && fx + 7*dx < ((SkFractionalInt) maxX << 32)) {
+                    SkFractionalInt rem = (((SkFractionalInt) maxX << 32) - 7*dx - fx - 1) / 8;
+                    int32_t rem_hi = rem >> 32;
+                    uint32_t rem_lo = (uint32_t) rem;
+                    int32_t fx_hi = fx >> 32;
+                    uint32_t fx_lo = (uint32_t) fx;
+                    __asm__ (
+                            "vmov        d16, %[fx_lo], %[fx_hi]     \n\t"
+                            "vmov        d24, %[dx_lo], %[dx_hi]     \n\t"
+                            "vadd.i64    d17, d16, d24               \n\t"
+                            "vmov        d25, %[dx_lo], %[dx_hi]     \n\t"
+                            "vmvn.i32    q13, #0x3fff                \n\t"
+                            "vadd.i64    d18, d17, d24               \n\t"
+                            "vmov.i32    q14, #1                     \n\t"
+                            "vadd.i64    d19, d18, d24               \n\t"
+                            "vshl.i64    q12, #2                     \n\t"
+                            "b           2f                          \n\t"
+                            "1:                                      \n\t"
+                            "vadd.i64    q8, q10, q12                \n\t"
+                            "vadd.i64    q9, q11, q12                \n\t"
+                            "2:                                      \n\t"
+                            "vadd.i64    q10, q8, q12                \n\t"
+                            "vadd.i64    q11, q9, q12                \n\t"
+                            "vshrn.i64   d16, q8, #14                \n\t"
+                            "vshrn.i64   d17, q9, #14                \n\t"
+                            "vand        q8, q13                     \n\t"
+                            "vorr        q8, q14                     \n\t"
+                            "vshrn.i64   d18, q10, #14               \n\t"
+                            "vshrn.i64   d19, q11, #14               \n\t"
+                            "vand        q9, q13                     \n\t"
+                            "subs        %[rem_lo], %[dx_lo]         \n\t"
+                            "vorr        q9, q14                     \n\t"
+                            "sbcs        %[rem_hi], %[dx_hi]         \n\t"
+                            "vsra.u32    q8, #18                     \n\t"
+                            "subs        %[count], #8                \n\t"
+                            "vsra.u32    q9, #18                     \n\t"
+                            "it          pl                          \n\t"
+                            "teqpl       %[rem_hi], #0               \n\t"
+                            "vst1.32     {q8-q9}, [%[dst]:128]!      \n\t"
+                            "bpl         1b                          \n\t"
+                            "vadd.i64    d16, d20, d24               \n\t"
+                            "vmov        %[fx_lo], %[fx_hi], d16     \n\t"
+                    : // Outputs
+                             [count]"+l"(count),
+                               [dst]"+r"(xy),
+                            [rem_hi]"+l"(rem_hi),
+                            [rem_lo]"+l"(rem_lo),
+                             [fx_hi]"+r"(fx_hi),
+                             [fx_lo]"+r"(fx_lo)
+                    : // Inputs
+                            [dx_hi]"l"((int32_t) (dx >> 32)),
+                            [dx_lo]"l"((uint32_t) dx)
+                    : // Clobbers
+                            "cc", "memory"
+                    );
+                    fx = ((SkFractionalInt) fx_hi << 32) | fx_lo;
+                }
+                count += 8-1;
+                while (count >= 0 && fx < ((SkFractionalInt) maxX << 32)) {
+                    *xy++ = ((uint32_t)(fx >> 14) & 0xffffc000) + (uint32_t)(fx >> 32) + 1;
+                    fx += dx;
+                    --count;
+                }
+                while (count >= 0) {
+                    *xy++ = (maxX << 18) + maxX;
+                    --count;
+                }
+            } else {
+                // Reflection case. Don't bother to optimize this as much -
+                // not even sure if it's used!
+                while (count >= 1 && fx >= ((SkFractionalInt) maxX << 32)) {
+                    *xy++ = (maxX << 18) + maxX;
+                    fx += dx;
+                    --count;
+                }
+                while (count >= 1 && fx >= 0) {
+                    *xy++ = ((uint32_t)(fx >> 14) & 0xffffc000) + (uint32_t)(fx >> 32) + 1;
+                    fx += dx;
+                    --count;
+                }
+                while (count >= 1) {
+                    *xy++ = 0;
+                    --count;
+                }
+            }
+        }
+        else
+        {
+        // Drop back to old code for repeat or other values of 'one'
+#endif
+        if (count >= 4) {
+            int32x4_t wide_fx;
+
+            wide_fx = vdupq_n_s32(SkFractionalIntToFixed(fx));
+            wide_fx = vsetq_lane_s32(SkFractionalIntToFixed(fx+dx), wide_fx, 1);
+            wide_fx = vsetq_lane_s32(SkFractionalIntToFixed(fx+dx+dx), wide_fx, 2);
+            wide_fx = vsetq_lane_s32(SkFractionalIntToFixed(fx+dx+dx+dx), wide_fx, 3);
+
+            while (count >= 4) {
+                int32x4_t res;
+
+                res = pack4(wide_fx, maxX, one);
+
+                vst1q_u32(xy, vreinterpretq_u32_s32(res));
+
+                wide_fx += vdupq_n_s32(SkFractionalIntToFixed(dx+dx+dx+dx));
+                fx += dx+dx+dx+dx;
+                xy += 4;
+                count -= 4;
+            }
+        }
+
+        while (--count >= 0) {
+            *xy++ = pack(SkFractionalIntToFixed(fx), maxX, one);
+            fx += dx;
+        }
+#ifndef __ARM_64BIT_STATE
+        }
+#endif
+    }
+
+    static const SkBitmapProcState::MatrixProc ClampX_ClampY_Procs[] = {
+        nofilter_scale_neon<clamp, clamp8, true>,
+        filter_scale_neon<clamp,
+                          clamp4,
+                          extract_low_bits_clamp_clamp,
+                          extract_low_bits_clamp4,
+                          true>,
+        nofilter_affine<clamp, clamp>,       filter_affine<clamp, clamp, extract_low_bits_clamp_clamp>,
+    };
+
+    static const SkBitmapProcState::MatrixProc RepeatX_RepeatY_Procs[] = {
+        nofilter_scale_neon<repeat, repeat8, false>,
+        filter_scale_neon<repeat,
+                          repeat4,
+                          extract_low_bits_general,
+                          extract_low_bits_repeat_mirror4,
+                          false>,
+        nofilter_affine<repeat, repeat>,        filter_affine<repeat, repeat, extract_low_bits_general>
+    };
+#else
+
 static const SkBitmapProcState::MatrixProc ClampX_ClampY_Procs[] = {
     nofilter_scale <clamp, clamp, true>, filter_scale <clamp, clamp, extract_low_bits_clamp_clamp, true>,
     nofilter_affine<clamp, clamp>,       filter_affine<clamp, clamp, extract_low_bits_clamp_clamp>,
@@ -280,6 +804,9 @@ static const SkBitmapProcState::MatrixPr
     nofilter_scale <repeat, repeat, false>, filter_scale <repeat, repeat, extract_low_bits_general, false>,
     nofilter_affine<repeat, repeat>,        filter_affine<repeat, repeat, extract_low_bits_general>
 };
+
+#endif
+
 static const SkBitmapProcState::MatrixProc MirrorX_MirrorY_Procs[] = {
     nofilter_scale <mirror, mirror,  false>, filter_scale <mirror, mirror, extract_low_bits_general, false>,
     nofilter_affine<mirror, mirror>,         filter_affine<mirror, mirror, extract_low_bits_general>,
--- a/src/third_party/skia/src/core/SkBitmapProcState_opts.cpp
+++ b/src/third_party/skia/src/core/SkBitmapProcState_opts.cpp
@@ -19,6 +19,9 @@
 
 namespace SkOpts {
     DEFINE_DEFAULT(S32_alpha_D32_filter_DX);
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+    DEFINE_DEFAULT(S32_opaque_D32_filter_DX);
+#endif
 
     void Init_BitmapProcState_ssse3();
 
--- a/src/third_party/skia/src/opts/SkBitmapProcState_opts.h
+++ b/src/third_party/skia/src/opts/SkBitmapProcState_opts.h
@@ -428,6 +428,255 @@ static void decode_packed_coordinates_an
                                                            __lsx_vsat_hu(sum, 8)), 0);
         }
     }
+#elif defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+
+#define S32_ALPHA_D32_FILTER_DX_1PIX_NEON(opt)               \
+            "ldr         %[x], [%[xy]], #4           \n\t"   \
+            "uxth        %[tmp2], %[x], ror #16      \n\t"   \
+            "lsl         %[tmp3], %[x], #2           \n\t"   \
+            "bic         %[tmp2], #3                 \n\t"   \
+            "uxth        %[tmp3], %[tmp3]            \n\t"   \
+            "add         %[tmp0], %[row0], %[tmp2]   \n\t"   \
+            "add         %[tmp1], %[row0], %[tmp3]   \n\t"   \
+            "add         %[tmp2], %[row1], %[tmp2]   \n\t"   \
+            "add         %[tmp3], %[row1], %[tmp3]   \n\t"   \
+            "lsr         %[x], #14                   \n\t"   \
+            "vldr        s0, [%[tmp0]]               \n\t"   \
+            "and         %[x], #0xf                  \n\t"   \
+            "vldr        s1, [%[tmp1]]               \n\t"   \
+            "vldr        s2, [%[tmp2]]               \n\t"   \
+            "vldr        s3, [%[tmp3]]               \n\t"   \
+            "vdup.16     d2, %[x]                    \n\t"   \
+            "vsub.i16    d3, d23, d2                 \n\t"   \
+            "vmull.u8    q2, d0, d31                 \n\t"   \
+            "vmlal.u8    q2, d1, d30                 \n\t"   \
+            "vmul.u16    d0, d4, d3                  \n\t"   \
+            "vmla.u16    d0, d5, d2                  \n\t"   \
+            "vshr.u16    d0, #8                      \n\t"   \
+            "vmul.u16    d0, d10                     \n\t"   \
+            opt"                                     \n\t"   \
+            "vshrn.u16   d0, q0, #8                  \n\t"   \
+            "vst1.32     {d0[0]}, [%[dst]:32]!       \n\t"   \
+
+void S32_alpha_D32_filter_DX(const SkBitmapProcState& s,
+                             const uint32_t* SK_RESTRICT xy,
+                             int count, SkPMColor* SK_RESTRICT colors) {
+    SkASSERT(count > 0 && colors != nullptr);
+    SkASSERT(4 == s.fPixmap.info().bytesPerPixel());
+    SkASSERT(s.fAlphaScale <= 256);
+
+    int y0, y1, wy;
+    decode_packed_coordinates_and_weight(*xy++, &y0, &y1, &wy);
+
+    auto row0 = (const uint32_t*)( (const char*)s.fPixmap.addr() + y0 * s.fPixmap.rowBytes() ),
+         row1 = (const uint32_t*)( (const char*)s.fPixmap.addr() + y1 * s.fPixmap.rowBytes() );
+
+    uint32_t tmp0, tmp1, tmp2, tmp3, x;
+    __asm__ volatile (
+            "vpush       {q4-q5}                     \n\t"
+            "vmov.i16    d22, #0xf                   \n\t"
+            "vmov.i16    d23, #0x10                  \n\t"
+            "vmov.i32    q12, #0x3fff                \n\t"
+            "vdup.32     q13, %[row0]                \n\t"
+            "vdup.32     q14, %[row1]                \n\t"
+            "vdup.i8     d30, %[subY]                \n\t"
+            "vmov.i8     d31, #16                    \n\t"
+            "vdup.16     q5, %[alpha]                \n\t"
+            "vshl.i32    q12, #2                     \n\t"
+            "tst         %[dst], #0xc                \n\t"
+            "vsub.i8     d31, d30                    \n\t"
+            "beq         2f                          \n\t"
+
+            "1:                                      \n\t"
+            S32_ALPHA_D32_FILTER_DX_1PIX_NEON(
+            "add         %[tmp0], %[dst], #4         \n\t"
+            "subs        %[len], #1                  \n\t"
+            "it          ne                          \n\t"
+            "tstne       %[tmp0], #0xc"
+            )
+            "bne         1b                          \n\t"
+
+            "2:"
+            "subs        %[len], #4                  \n\t"
+            "bmi         13f                         \n\t"
+
+            "vld1.32     {q8}, [%[xy]]!              \n\t"
+            "vshr.u32    q9, q8, #16                 \n\t"
+            "vand        q9, q12                     \n\t"
+            "vadd.i32    q1, q13, q9                 \n\t"
+            "vshl.i32    q0, q8, #2                  \n\t"
+            "vand        q0, q12                     \n\t"
+            "vadd.i32    q2, q13, q0                 \n\t"
+            "vmov        %[tmp0], s4                 \n\t"
+            "vmov        %[tmp1], s5                 \n\t"
+            "vadd.i32    q3, q14, q9                 \n\t"
+            "vmov        %[tmp2], %[tmp3], d3        \n\t"
+
+            "11:                                     \n\t"
+            "vadd.i32    q4, q14, q0                 \n\t"
+            "vldr        s4, [%[tmp0]]               \n\t"
+            "vmov        %[tmp0], s8                 \n\t"
+            "vldr        s5, [%[tmp1]]               \n\t"
+            "vmov        %[tmp1], s9                 \n\t"
+            "vldr        s6, [%[tmp2]]               \n\t"
+            "vmov        %[tmp2], s10                \n\t"
+            "vldr        s7, [%[tmp3]]               \n\t"
+            "vmov        %[tmp3], s11                \n\t"
+            "vldr        s8, [%[tmp0]]               \n\t"
+            "vmov        %[tmp0], s12                \n\t"
+            "vldr        s9, [%[tmp1]]               \n\t"
+            "vmov        %[tmp1], s13                \n\t"
+            "vldr        s10, [%[tmp2]]              \n\t"
+            "vmov        %[tmp2], s14                \n\t"
+            "vldr        s11, [%[tmp3]]              \n\t"
+            "vmov        %[tmp3], s15                \n\t"
+            "vldr        s12, [%[tmp0]]              \n\t"
+            "vmov        %[tmp0], s16                \n\t"
+            "vldr        s13, [%[tmp1]]              \n\t"
+            "vmov        %[tmp1], s17                \n\t"
+            "vldr        s14, [%[tmp2]]              \n\t"
+            "vmov        %[tmp2], s18                \n\t"
+            "vldr        s15, [%[tmp3]]              \n\t"
+            "vmov        %[tmp3], s19                \n\t"
+            "vldr        s16, [%[tmp0]]              \n\t"
+            "vshrn.i32   d1, q8, #14                 \n\t"
+            "vldr        s17, [%[tmp1]]              \n\t"
+            "vand        d1, d22                     \n\t"
+            "vldr        s18, [%[tmp2]]              \n\t"
+            "vsub.i16    d0, d23, d1                 \n\t"
+            "vldr        s19, [%[tmp3]]              \n\t"
+            "vmull.u8    q10, d2, d31                \n\t"
+            "vmlal.u8    q10, d6, d30                \n\t"
+            "vmull.u8    q1, d3, d31                 \n\t"
+            "vmlal.u8    q1, d7, d30                 \n\t"
+            "vmull.u8    q3, d4, d31                 \n\t"
+            "subs        %[len], #4                  \n\t"
+            "vmlal.u8    q3, d8, d30                 \n\t"
+            "bmi         12f                         \n\t"
+
+            "  vld1.32     {q8}, [%[xy]]!            \n\t"
+            "vmull.u8    q2, d5, d31                 \n\t"
+            "vmlal.u8    q2, d9, d30                 \n\t"
+            "vmul.u16    d8, d20, d0[0]              \n\t"
+            "  vshr.u32    d18, d16, #16             \n\t"
+            "vmul.u16    d9, d21, d0[1]              \n\t"
+            "  vshr.u32    d19, d17, #16             \n\t"
+            "vmul.u16    d20, d2, d0[2]              \n\t"
+            "  vand        d18, d24                  \n\t"
+            "vmul.u16    d21, d3, d0[3]              \n\t"
+            "  vand        d19, d25                  \n\t"
+            "vmla.u16    d8, d6, d1[0]               \n\t"
+            "  vadd.i32    d2, d26, d18              \n\t"
+            "vmla.u16    d9, d7, d1[1]               \n\t"
+            "  vadd.i32    d3, d27, d19              \n\t"
+            "vmla.u16    d20, d4, d1[2]              \n\t"
+            "  vshl.i32    d0, d16, #2               \n\t"
+            "vmla.u16    d21, d5, d1[3]              \n\t"
+            "  vshl.i32    d1, d17, #2               \n\t"
+            "  vand        q0, q12                   \n\t"
+            "  vadd.i32    q2, q13, q0               \n\t"
+            "vshr.u16    q4, #8                      \n\t"
+            "vshr.u16    q10, #8                     \n\t"
+            "vmul.u16    q4, q5                      \n\t"
+            "vmul.u16    q10, q5                     \n\t"
+            "  vmov        %[tmp0], %[tmp1], d2      \n\t"
+            "  vadd.i32    q3, q14, q9               \n\t"
+            "  vmov        %[tmp2], %[tmp3], d3      \n\t"
+            "vshrn.u16   d8, q4, #8                  \n\t"
+            "vshrn.u16   d9, q10, #8                 \n\t"
+            "vst1.32     {q4}, [%[dst]:128]!         \n\t"
+            "b           11b                         \n\t"
+
+            "12:                                     \n\t"
+            "vmull.u8    q2, d5, d31                 \n\t"
+            "vmlal.u8    q2, d9, d30                 \n\t"
+            "vmul.u16    d8, d20, d0[0]              \n\t"
+            "vmul.u16    d9, d21, d0[1]              \n\t"
+            "vmul.u16    d20, d2, d0[2]              \n\t"
+            "vmul.u16    d21, d3, d0[3]              \n\t"
+            "vmla.u16    d8, d6, d1[0]               \n\t"
+            "vmla.u16    d9, d7, d1[1]               \n\t"
+            "vmla.u16    d20, d4, d1[2]              \n\t"
+            "vmla.u16    d21, d5, d1[3]              \n\t"
+            "vshr.u16    q4, #8                      \n\t"
+            "vshr.u16    q10, #8                     \n\t"
+            "vmul.u16    q4, q5                      \n\t"
+            "vmul.u16    q10, q5                     \n\t"
+            "vshrn.u16   d8, q4, #8                  \n\t"
+            "vshrn.u16   d9, q10, #8                 \n\t"
+            "vst1.32     {q4}, [%[dst]:128]!         \n\t"
+
+            "13:                                     \n\t"
+            "adds        %[len], #4-1                \n\t"
+            "bmi         22f                         \n\t"
+
+            "21:                                     \n\t"
+            S32_ALPHA_D32_FILTER_DX_1PIX_NEON("subs %[len], #1")
+            "bpl         21b                         \n\t"
+
+            "22:                                     \n\t"
+            "vpop        {q4-q5}                     \n\t"
+    : // Outputs
+             [dst]"+r"(colors),
+              [xy]"+r"(xy),
+             [len]"+r"(count),
+            [tmp0]"=&r"(tmp0),
+            [tmp1]"=&r"(tmp1),
+            [tmp2]"=&r"(tmp2),
+            [tmp3]"=&r"(tmp3),
+               [x]"=&r"(x)
+    : // Inputs
+            [alpha]"r"(s.fAlphaScale),
+             [row0]"r"(row0),
+             [row1]"r"(row1),
+             [subY]"r"(wy)
+    : // Clobbers
+            "cc", "memory"
+    );
+}
+#if 0
+        // Copied from just below
+        static void filter_and_scale_by_alpha(unsigned x, unsigned y,
+                                              SkPMColor a00, SkPMColor a01,
+                                              SkPMColor a10, SkPMColor a11,
+                                              SkPMColor *dst,
+                                              uint16_t scale) {
+            uint8x8_t vy, vconst16_8, v16_y, vres;
+            uint16x4_t vx, vconst16_16, v16_x, tmp, vscale;
+            uint32x2_t va0, va1;
+            uint16x8_t tmp1, tmp2;
+
+            vy = vdup_n_u8(y);                // duplicate y into vy
+            vconst16_8 = vmov_n_u8(16);       // set up constant in vconst16_8
+            v16_y = vsub_u8(vconst16_8, vy);  // v16_y = 16-y
+
+            va0 = vdup_n_u32(a00);            // duplicate a00
+            va1 = vdup_n_u32(a10);            // duplicate a10
+            va0 = vset_lane_u32(a01, va0, 1); // set top to a01
+            va1 = vset_lane_u32(a11, va1, 1); // set top to a11
+
+            tmp1 = vmull_u8(vreinterpret_u8_u32(va0), v16_y); // tmp1 = [a01|a00] * (16-y)
+            tmp2 = vmull_u8(vreinterpret_u8_u32(va1), vy);    // tmp2 = [a11|a10] * y
+
+            vx = vdup_n_u16(x);                // duplicate x into vx
+            vconst16_16 = vmov_n_u16(16);      // set up constant in vconst16_16
+            v16_x = vsub_u16(vconst16_16, vx); // v16_x = 16-x
+
+            tmp = vmul_u16(vget_high_u16(tmp1), vx);        // tmp  = a01 * x
+            tmp = vmla_u16(tmp, vget_high_u16(tmp2), vx);   // tmp += a11 * x
+            tmp = vmla_u16(tmp, vget_low_u16(tmp1), v16_x); // tmp += a00 * (16-x)
+            tmp = vmla_u16(tmp, vget_low_u16(tmp2), v16_x); // tmp += a10 * (16-x)
+
+            if (scale < 256) {
+                vscale = vdup_n_u16(scale);        // duplicate scale
+                tmp = vshr_n_u16(tmp, 8);          // shift down result by 8
+                tmp = vmul_u16(tmp, vscale);       // multiply result by scale
+            }
+
+            vres = vshrn_n_u16(vcombine_u16(tmp, vcreate_u16(0)), 8); // shift down result by 8
+            vst1_lane_u32(dst, vreinterpret_u32_u8(vres), 0);         // store result
+        }
+#endif
 
 #else
 
@@ -541,6 +790,202 @@ static void decode_packed_coordinates_an
 
 #endif
 
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+
+#define S32_OPAQUE_D32_FILTER_DX_1PIX_NEON(opt)              \
+            "ldr         %[x], [%[xy]], #4           \n\t"   \
+            "uxth        %[tmp2], %[x], ror #16      \n\t"   \
+            "lsl         %[tmp3], %[x], #2           \n\t"   \
+            "bic         %[tmp2], #3                 \n\t"   \
+            "uxth        %[tmp3], %[tmp3]            \n\t"   \
+            "add         %[tmp0], %[row0], %[tmp2]   \n\t"   \
+            "add         %[tmp1], %[row0], %[tmp3]   \n\t"   \
+            "add         %[tmp2], %[row1], %[tmp2]   \n\t"   \
+            "add         %[tmp3], %[row1], %[tmp3]   \n\t"   \
+            "lsr         %[x], #14                   \n\t"   \
+            "vldr        s0, [%[tmp0]]               \n\t"   \
+            "and         %[x], #0xf                  \n\t"   \
+            "vldr        s1, [%[tmp1]]               \n\t"   \
+            "vldr        s2, [%[tmp2]]               \n\t"   \
+            "vldr        s3, [%[tmp3]]               \n\t"   \
+            "vdup.16     d2, %[x]                    \n\t"   \
+            "vsub.i16    d3, d23, d2                 \n\t"   \
+            "vmull.u8    q2, d0, d31                 \n\t"   \
+            "vmlal.u8    q2, d1, d30                 \n\t"   \
+            "vmul.u16    d0, d4, d3                  \n\t"   \
+            "vmla.u16    d0, d5, d2                  \n\t"   \
+            opt"                                     \n\t"   \
+            "vshrn.u16   d0, q0, #8                  \n\t"   \
+            "vst1.32     {d0[0]}, [%[dst]:32]!       \n\t"   \
+
+void S32_opaque_D32_filter_DX(const SkBitmapProcState& s,
+                              const uint32_t* SK_RESTRICT xy,
+                              int count, SkPMColor* SK_RESTRICT colors) {
+    SkASSERT(count > 0 && colors != nullptr);
+    SkASSERT(4 == s.fPixmap.info().bytesPerPixel());
+    SkASSERT(s.fAlphaScale == 256);
+
+    int y0, y1, wy;
+    decode_packed_coordinates_and_weight(*xy++, &y0, &y1, &wy);
+
+    auto row0 = (const uint32_t*)( (const char*)s.fPixmap.addr() + y0 * s.fPixmap.rowBytes() ),
+         row1 = (const uint32_t*)( (const char*)s.fPixmap.addr() + y1 * s.fPixmap.rowBytes() );
+
+    uint32_t tmp0, tmp1, tmp2, tmp3, x;
+    __asm__ volatile (
+            "vpush       {q4}                        \n\t"
+            "vmov.i16    d22, #0xf                   \n\t"
+            "vmov.i16    d23, #0x10                  \n\t"
+            "vmov.i32    q12, #0x3fff                \n\t"
+            "vdup.32     q13, %[row0]                \n\t"
+            "vdup.32     q14, %[row1]                \n\t"
+            "vdup.i8     d30, %[subY]                \n\t"
+            "vmov.i8     d31, #16                    \n\t"
+            "vshl.i32    q12, #2                     \n\t"
+            "tst         %[dst], #0xc                \n\t"
+            "vsub.i8     d31, d30                    \n\t"
+            "beq         2f                          \n\t"
+
+            "1:                                      \n\t"
+            S32_OPAQUE_D32_FILTER_DX_1PIX_NEON(
+            "add         %[tmp0], %[dst], #4         \n\t"
+            "subs        %[len], #1                  \n\t"
+            "it          ne                          \n\t"
+            "tstne       %[tmp0], #0xc"
+            )
+            "bne         1b                          \n\t"
+
+            "2:"
+            "subs        %[len], #4                  \n\t"
+            "bmi         13f                         \n\t"
+
+            "vld1.32     {q8}, [%[xy]]!              \n\t"
+            "vshr.u32    q9, q8, #16                 \n\t"
+            "vand        q9, q12                     \n\t"
+            "vadd.i32    q1, q13, q9                 \n\t"
+            "vshl.i32    q0, q8, #2                  \n\t"
+            "vand        q0, q12                     \n\t"
+            "vadd.i32    q2, q13, q0                 \n\t"
+            "vmov        %[tmp0], s4                 \n\t"
+            "vmov        %[tmp1], s5                 \n\t"
+
+            "11:                                     \n\t"
+            "vadd.i32    q3, q14, q9                 \n\t"
+            "vmov        %[tmp2], %[tmp3], d3        \n\t"
+            "vadd.i32    q4, q14, q0                 \n\t"
+            "vldr        s4, [%[tmp0]]               \n\t"
+            "vmov        %[tmp0], s8                 \n\t"
+            "vldr        s5, [%[tmp1]]               \n\t"
+            "vmov        %[tmp1], s9                 \n\t"
+            "vldr        s6, [%[tmp2]]               \n\t"
+            "vmov        %[tmp2], s10                \n\t"
+            "vldr        s7, [%[tmp3]]               \n\t"
+            "vmov        %[tmp3], s11                \n\t"
+            "vldr        s8, [%[tmp0]]               \n\t"
+            "vmov        %[tmp0], s12                \n\t"
+            "vldr        s9, [%[tmp1]]               \n\t"
+            "vmov        %[tmp1], s13                \n\t"
+            "vldr        s10, [%[tmp2]]              \n\t"
+            "vmov        %[tmp2], s14                \n\t"
+            "vldr        s11, [%[tmp3]]              \n\t"
+            "vmov        %[tmp3], s15                \n\t"
+            "vldr        s12, [%[tmp0]]              \n\t"
+            "vmov        %[tmp0], s16                \n\t"
+            "vldr        s13, [%[tmp1]]              \n\t"
+            "vmov        %[tmp1], s17                \n\t"
+            "vldr        s14, [%[tmp2]]              \n\t"
+            "vmov        %[tmp2], s18                \n\t"
+            "vldr        s15, [%[tmp3]]              \n\t"
+            "vmov        %[tmp3], s19                \n\t"
+            "vldr        s16, [%[tmp0]]              \n\t"
+            "vshrn.i32   d1, q8, #14                 \n\t"
+            "vldr        s17, [%[tmp1]]              \n\t"
+            "vand        d1, d22                     \n\t"
+            "vldr        s18, [%[tmp2]]              \n\t"
+            "vsub.i16    d0, d23, d1                 \n\t"
+            "vldr        s19, [%[tmp3]]              \n\t"
+            "vmull.u8    q10, d2, d31                \n\t"
+            "vmlal.u8    q10, d6, d30                \n\t"
+            "vmull.u8    q1, d3, d31                 \n\t"
+            "vmlal.u8    q1, d7, d30                 \n\t"
+            "vmull.u8    q3, d4, d31                 \n\t"
+            "subs        %[len], #4                  \n\t"
+            "vmlal.u8    q3, d8, d30                 \n\t"
+            "bmi         12f                         \n\t"
+
+            "  vld1.32     {q8}, [%[xy]]!            \n\t"
+            "vmull.u8    q2, d5, d31                 \n\t"
+            "vmlal.u8    q2, d9, d30                 \n\t"
+            "vmul.u16    d8, d20, d0[0]              \n\t"
+            "  vshr.u32    d18, d16, #16             \n\t"
+            "vmul.u16    d9, d21, d0[1]              \n\t"
+            "  vshr.u32    d19, d17, #16             \n\t"
+            "vmul.u16    d20, d2, d0[2]              \n\t"
+            "  vand        d18, d24                  \n\t"
+            "vmul.u16    d21, d3, d0[3]              \n\t"
+            "  vand        d19, d25                  \n\t"
+            "vmla.u16    d8, d6, d1[0]               \n\t"
+            "  vadd.i32    d2, d26, d18              \n\t"
+            "vmla.u16    d9, d7, d1[1]               \n\t"
+            "  vadd.i32    d3, d27, d19              \n\t"
+            "vmla.u16    d20, d4, d1[2]              \n\t"
+            "  vshl.i32    d0, d16, #2               \n\t"
+            "vmla.u16    d21, d5, d1[3]              \n\t"
+            "  vshl.i32    d1, d17, #2               \n\t"
+            "  vand        q0, q12                   \n\t"
+            "  vadd.i32    q2, q13, q0               \n\t"
+            "  vmov        %[tmp0], s4               \n\t"
+            "  vmov        %[tmp1], s5               \n\t"
+            "vshrn.u16   d8, q4, #8                  \n\t"
+            "vshrn.u16   d9, q10, #8                 \n\t"
+            "vst1.32     {q4}, [%[dst]:128]!         \n\t"
+            "b           11b                         \n\t"
+
+            "12:                                     \n\t"
+            "vmull.u8    q2, d5, d31                 \n\t"
+            "vmlal.u8    q2, d9, d30                 \n\t"
+            "vmul.u16    d8, d20, d0[0]              \n\t"
+            "vmul.u16    d9, d21, d0[1]              \n\t"
+            "vmul.u16    d20, d2, d0[2]              \n\t"
+            "vmul.u16    d21, d3, d0[3]              \n\t"
+            "vmla.u16    d8, d6, d1[0]               \n\t"
+            "vmla.u16    d9, d7, d1[1]               \n\t"
+            "vmla.u16    d20, d4, d1[2]              \n\t"
+            "vmla.u16    d21, d5, d1[3]              \n\t"
+            "vshrn.u16   d8, q4, #8                  \n\t"
+            "vshrn.u16   d9, q10, #8                 \n\t"
+            "vst1.32     {q4}, [%[dst]:128]!         \n\t"
+
+            "13:                                     \n\t"
+            "adds        %[len], #4-1                \n\t"
+            "bmi         22f                         \n\t"
+
+            "21:                                     \n\t"
+            S32_OPAQUE_D32_FILTER_DX_1PIX_NEON("subs %[len], #1")
+            "bpl         21b                         \n\t"
+
+            "22:                                     \n\t"
+            "vpop        {q4}                        \n\t"
+    : // Outputs
+             [dst]"+r"(colors),
+              [xy]"+r"(xy),
+             [len]"+r"(count),
+            [tmp0]"=&r"(tmp0),
+            [tmp1]"=&r"(tmp1),
+            [tmp2]"=&r"(tmp2),
+            [tmp3]"=&r"(tmp3),
+               [x]"=&r"(x)
+    : // Inputs
+            [row0]"r"(row0),
+            [row1]"r"(row1),
+            [subY]"r"(wy)
+    : // Clobbers
+            "cc", "memory"
+    );
+}
+
+#endif
+
 }  // namespace SK_OPTS_NS
 
 namespace sktests {
--- a/src/third_party/skia/src/opts/SkBlitMask_opts.h
+++ b/src/third_party/skia/src/opts/SkBlitMask_opts.h
@@ -409,6 +409,322 @@ namespace SK_OPTS_NS {
     }
 }
 
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+
+// These macros permit optionally-included features to be switched using a parameter to another macro
+#define YES(x) x
+#define NO(x)
+
+// How far ahead (pixels) to preload (undefine to disable prefetch) - determined empirically
+#define PREFETCH_DISTANCE "52"
+
+#ifdef PREFETCH_DISTANCE
+#define IF_PRELOAD YES
+#else
+#define IF_PRELOAD NO
+#endif
+
+/// Macro to load 1..7 source and mask pixels in growing powers-of-2 in size - suitable for leading pixels
+#define S32A_A8_LOAD_SM_LEADING_7(r0, r1, r2, r3, s_base, r4, m_base, opt1, opt2)                       \
+                opt1"                                                                           \n\t"   \
+                "tst         %[group_size], #1                                                  \n\t"   \
+                opt2"                                                                           \n\t"   \
+                "beq         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[1]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+                "lsls        %[tmp], %[group_size], #30                                         \n\t"   \
+                "bpl         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[2]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[3]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+                "bcc         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[4]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[5]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[6]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[7]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[7],"#r1"[7],"#r2"[7],"#r3"[7]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+
+/// Macro to load or store 1..7 destination pixels in growing powers-of-2 in size - suitable for leading pixels
+#define S32A_A8_LOADSTORE_D_LEADING_7(ls, r0, r1, r2, r3, d_base)                                       \
+                "tst         %[group_size], #1                                                  \n\t"   \
+                "beq         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%["#d_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+                "lsls        %[tmp], %[group_size], #30                                         \n\t"   \
+                "add         %[tmp], %["#d_base"], #4                                           \n\t"   \
+                "bpl         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "1:                                                                             \n\t"   \
+                "bcc         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "v"#ls"4.8   {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[7],"#r1"[7],"#r2"[7],"#r3"[7]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "1:                                                                             \n\t"   \
+
+/// Macro to load 1..7 source and mask pixels in shrinking powers-of-2 in size - suitable for trailing pixels
+#define S32A_A8_LOAD_SM_TRAILING_7(r0, r1, r2, r3, s_base, r4, m_base, opt1, opt2)                      \
+                opt1"                                                                           \n\t"   \
+                "lsls        %[tmp], %[group_size], #30                                         \n\t"   \
+                opt2"                                                                           \n\t"   \
+                "bcc         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[0]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[0],"#r1"[0],"#r2"[0],"#r3"[0]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[1]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[2]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[3]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+                "bpl         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[4]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#s_base"]:32]!          \n\t"   \
+                "vld1.8      {"#r4"[5]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+                "tst         %[group_size], #1                                                  \n\t"   \
+                "beq         1f                                                                 \n\t"   \
+                "vld1.8      {"#r4"[6]}, [%["#m_base"]]!                                        \n\t"   \
+                "vld4.8      {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#s_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+
+/// Macro to load or store 1..7 destination pixels in shrinking powers-of-2 in size - suitable for trailing pixels
+#define S32A_A8_LOADSTORE_D_TRAILING_7(ls, r0, r1, r2, r3, d_base)                                      \
+                "lsls        %[tmp], %[group_size], #30                                         \n\t"   \
+                "add         %[tmp], %["#d_base"], #4                                           \n\t"   \
+                "bcc         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[0],"#r1"[0],"#r2"[0],"#r3"[0]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "v"#ls"4.8   {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "1:                                                                             \n\t"   \
+                "bpl         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#d_base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%[tmp]:32], %[eight]       \n\t"   \
+                "1:                                                                             \n\t"   \
+                "tst         %[group_size], #1                                                  \n\t"   \
+                "beq         1f                                                                 \n\t"   \
+                "v"#ls"4.8   {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#d_base"]:32]!          \n\t"   \
+                "1:                                                                             \n\t"   \
+
+/// Macro to do shortcut testing for "over" compositing of 32bpp premultiplied ARGB source and 8-bit alpha mask
+#define S32A_A8_TEST(dst_adjust)                                                                        \
+                "vmov        %[mlo], %[mhi], d16                                                \n\t"   \
+                "vmov        %[alo], s6                                                         \n\t"   \
+                "vmov        %[ahi], s7                                                         \n\t"   \
+                "and         %[tmp], %[mlo], %[mhi]                                             \n\t"   \
+                "orrs        %[mlo], %[mhi]                                                     \n\t"   \
+                "it          ne                                                                 \n\t"   \
+                "orrsne      %[mlo], %[alo], %[ahi]                                             \n\t"   \
+                "it          eq                                                                 \n\t"   \
+                "addeq       %[dst], " dst_adjust "                                             \n\t"   \
+                "beq         9f                                                                 \n\t"   \
+                "and         %[tmp], %[alo]                                                     \n\t"   \
+                "and         %[tmp], %[ahi]                                                     \n\t"   \
+                "cmp         %[tmp], #-1                                                        \n\t"   \
+                "beq         5f                                                                 \n\t"   \
+
+/// Macro to do testing and "over" compositing of a group of 1..7 32bpp premultiplied ARGB source and 1..7 8-bit alpha mask leading or trailing pixels
+#define S32A_A8_7PIX_PROCESS(load_sm_7, loadstore_d_7, size)                                            \
+    do {                                                                                                \
+        __asm__ volatile (                                                                              \
+                /* Load the leading/trailing source pixels,                                             \
+                 * after initialising all the unused indexes from the first pixel                       \
+                 * so the all-opaque and all-transparent tests still work */                            \
+                load_sm_7(d0, d1, d2, d3, src, d16, msk,                                                \
+                "vld1.8      {d16[]}, [%[msk]]",                                                        \
+                "vld4.8      {d0[], d1[], d2[], d3[]}, [%[src]]")                                       \
+                S32A_A8_TEST("%[group_size], lsl #2")                                                   \
+                /* Translucency used, or a mixture of opaque and transparent */                         \
+                loadstore_d_7(ld, d4, d5, d6, d7, dst)                                                  \
+                "sub         %[dst], %[group_size], lsl #2                                      \n\t"   \
+                S32A_A8_8PIX_BLEND(, NO, NO)                                                      \
+                loadstore_d_7(st, d0, d1, d2, d3, dst)                                                  \
+                /* Drop through */                                                                      \
+                "9:                                                                             \n\t"   \
+        : /* Outputs */                                                                                 \
+                [mlo]"=&r"(mlo),                                                                        \
+                [mhi]"=&r"(mhi),                                                                        \
+                [alo]"=&r"(alo),                                                                        \
+                [ahi]"=&r"(ahi),                                                                        \
+                [tmp]"=&r"(tmp),                                                                        \
+                [src]"+r"(src),                                                                         \
+                [msk]"+r"(msk),                                                                         \
+                [dst]"+r"(dst)                                                                          \
+        : /* Inputs */                                                                                  \
+                [group_size]"r"(size),                                                                  \
+                     [eight]"r"(eight)                                                                  \
+        : /* Clobbers */                                                                                \
+                "cc", "memory"                                                                          \
+        );                                                                                              \
+    } while (0)
+
+/// Macro to do "over" compositing blending on 8 32bpp premultiplied ARGB source and 8 8-bit alpha mask pixels
+/// which are with either translucent or a mixture of opaque and transparent.
+/// Relies on A(x) to determine whether to emit code in ARM state (as opposed to Thumb state).
+/// @arg align           bit-alignment specifier on destination loads/stores (optional)
+/// @arg if_loadstore    YES or NO: whether to do load/store of destination
+/// @arg if_preload      YES or NO: whether to insert prefetch instructions
+#define S32A_A8_8PIX_BLEND(align, if_loadstore, if_preload)                                        \
+if_loadstore(   "vld4.8      {d4-d7}, [%[dst]"#align"]                                          \n\t")  \
+if_preload(     "sub         %[tmp], %[len], #1                                                 \n\t")  \
+                "vmull.u8    q9, d3, d16                                                        \n\t"   \
+if_preload(     "cmp         %[tmp], #" PREFETCH_DISTANCE "                                     \n\t")  \
+                "vmull.u8    q10, d0, d16                                                       \n\t"   \
+if_preload(     "it          cs                                                                 \n\t")  \
+if_preload(     "movcs       %[tmp], #" PREFETCH_DISTANCE "                                     \n\t")  \
+                "vmull.u8    q11, d1, d16                                                       \n\t"   \
+                "vmull.u8    q8, d2, d16                                                        \n\t"   \
+                "vrshr.u16   q1, q9, #8                                                         \n\t"   \
+if_preload(     "pld         [%[msk], %[tmp]]                                                   \n\t")  \
+                "vrshr.u16   q0, q10, #8                                                        \n\t"   \
+if_preload(     "pld         [%[src], %[tmp], lsl #2]                                           \n\t")  \
+                "vraddhn.u16 d3, q9, q1                                                         \n\t"   \
+if_preload(     "add         %[tmp], #32/4                                                      \n\t")  \
+                "vrshr.u16   q9, q11, #8                                                        \n\t"   \
+                "vrshr.u16   q12, q8, #8                                                        \n\t"   \
+                "vmvn        d2, d3                                                             \n\t"   \
+if_preload(     "pld         [%[dst], %[tmp], lsl #2]                                           \n\t")  \
+                "vraddhn.u16 d0, q10, q0                                                        \n\t"   \
+                "vmull.u8    q10, d4, d2                                                        \n\t"   \
+                "vmull.u8    q13, d5, d2                                                        \n\t"   \
+                "vmull.u8    q14, d6, d2                                                        \n\t"   \
+                "vmull.u8    q15, d7, d2                                                        \n\t"   \
+                "vrshr.u16   q2, q10, #8                                                        \n\t"   \
+                "vrshr.u16   q3, q13, #8                                                        \n\t"   \
+                "vraddhn.u16 d1, q11, q9                                                        \n\t"   \
+                "vrshr.u16   q9, q14, #8                                                        \n\t"   \
+                "vrshr.u16   q11, q15, #8                                                       \n\t"   \
+                "vraddhn.u16 d4, q10, q2                                                        \n\t"   \
+                "vraddhn.u16 d5, q13, q3                                                        \n\t"   \
+                "vraddhn.u16 d2, q8, q12                                                        \n\t"   \
+                "vraddhn.u16 d6, q14, q9                                                        \n\t"   \
+                "vraddhn.u16 d7, q15, q11                                                       \n\t"   \
+                "vadd.u8     q0, q2                                                             \n\t"   \
+                "vadd.u8     q1, q3                                                             \n\t"   \
+                "5:                                                                             \n\t"   \
+if_loadstore(   "vst4.8      {d0-d3}, [%[dst]"#align"]!                                         \n\t")  \
+
+#endif
+
+/*not static*/ inline
+void blit_row_s32a_a8(SkPMColor* dst, const void* vmask, const SkPMColor* src, int n) {
+#if defined(SK_ARM_HAS_NEON) && !defined(__ARM_64BIT_STATE)
+    const SkAlpha* msk = static_cast<const SkAlpha*>(vmask);
+    uint32_t tmp, mlo, mhi, alo, ahi;
+    const int eight = 8;
+    if (n < 15) {
+        // Too short to attempt aligned processing
+        if (n & 8) {
+            __asm__ (
+                    "vld1.8      {d16}, [%[msk]]!                                                   \n\t"
+                    "vld4.8      {d0-d3}, [%[src]]!                                                 \n\t"
+                    S32A_A8_TEST("#8*4")
+                    /* Translucency used, or a mixture of opaque and transparent */
+                    S32A_A8_8PIX_BLEND(, YES, NO)
+                    /* Drop through */
+                    "9:                                                                             \n\t"
+            :  /* Outputs */
+                    [mlo]"=&r"(mlo),
+                    [mhi]"=&r"(mhi),
+                    [alo]"=&r"(alo),
+                    [ahi]"=&r"(ahi),
+                    [tmp]"=&r"(tmp),
+                    [src]"+r"(src),
+                    [msk]"+r"(msk),
+                    [dst]"+r"(dst)
+            : /* Inputs */
+            : /* Clobbers */
+                    "cc", "memory"
+            );
+        }
+        if (n & 7)
+            S32A_A8_7PIX_PROCESS(S32A_A8_LOAD_SM_TRAILING_7, S32A_A8_LOADSTORE_D_TRAILING_7, n & 7);
+    } else {
+        // The last 0 - 7 pixels (starting from a 4-pixel boundary) are handled together
+        uintptr_t startrup = (uintptr_t) dst / sizeof (*dst) + 3;
+        uintptr_t end = (uintptr_t) dst / sizeof (*dst) + n;
+        size_t trailing;
+        if ((end & 3) == 0)
+            // No blocks of <8 pixels used at end in these cases
+            trailing = 0;
+        else
+            // If length (discounting alignment to 4-pixel boundaries) is an odd number of 4-pixels,
+            // assign 4 pixels to trailing end to avoid possibility of a leading run of exactly 4,
+            // otherwise use <4 trailing pixels to maximise central 8-pixel blocks
+            trailing = ((startrup ^ end) & 4) + (end & 3);
+        // The inner loop handles an integer number (0+) of 8-pixel blocks at 4-pixel boundaries
+        // The 0 - 7 pixels leading up to this are handled together
+        size_t leading = (n - trailing) & 7;
+
+        // Do leading pixels
+        if (leading != 0) {
+            n -= leading;
+            S32A_A8_7PIX_PROCESS(S32A_A8_LOAD_SM_LEADING_7, S32A_A8_LOADSTORE_D_LEADING_7, leading);
+        }
+
+        // Do inner loop
+        __asm__ (
+                "subs        %[len], #8                                                         \n\t"
+                "bcc         50f                                                                \n\t"
+
+                "10:                                                                            \n\t"
+                "vld1.8      {d16}, [%[msk]]!                                                   \n\t"
+                "vld4.8      {d0-d3}, [%[src]]!                                                 \n\t"
+                S32A_A8_TEST("#8*4")
+                /* Translucency used, or a mixture of opaque and transparent */
+                S32A_A8_8PIX_BLEND(:128, YES, IF_PRELOAD)
+                /* Drop through */
+                "9:                                                                             \n\t"
+                "subs        %[len], #8                                                         \n\t"
+                "bcs         10b                                                                \n\t"
+                "50:                                                                            \n\t"
+        : // Outputs
+                [mlo]"=&r"(mlo),
+                [mhi]"=&r"(mhi),
+                [alo]"=&r"(alo),
+                [ahi]"=&r"(ahi),
+                [tmp]"=&r"(tmp),
+                [src]"+r"(src),
+                [msk]"+r"(msk),
+                [dst]"+r"(dst),
+                [len]"+r"(n)
+        : // Inputs
+        : // Clobbers
+                "cc", "memory"
+        );
+
+        // Do trailing pixels.
+        if (n & 7)
+            S32A_A8_7PIX_PROCESS(S32A_A8_LOAD_SM_TRAILING_7, S32A_A8_LOADSTORE_D_TRAILING_7, n & 7);
+    }
+#else
+    auto mask = (const uint8_t*)vmask;
+
+#ifdef SK_SUPPORT_LEGACY_A8_MASKBLITTER
+    for (int i = 0; i < n; ++i) {
+        if (mask[i]) {
+            dst[i] = SkBlendARGB32(src[i], dst[i], mask[i]);
+        }
+    }
+#else
+    Sk4px::MapDstSrcAlpha(n, dst, src, mask, [](const Sk4px& d, const Sk4px& s, const Sk4px& aa) {
+        const auto s_aa = s.approxMulDiv255(aa);
+        return s_aa + d.approxMulDiv255(s_aa.alphas().inv());
+    });
+#endif
+#endif
+}
+
 }  // namespace SK_OPTS_NS
 
 #endif//SkBlitMask_opts_DEFINED
--- a/src/third_party/skia/src/opts/SkBlitRow_opts.h
+++ b/src/third_party/skia/src/opts/SkBlitRow_opts.h
@@ -89,6 +89,10 @@
 #endif
 
 #if defined(SK_ARM_HAS_NEON)
+#ifdef __ARM_64BIT_STATE
+// No attempt has been made to adapt the inline assembly version for AArch64
+// so fall back to the less performant version that uses intrinsics instead
+
     #include <arm_neon.h>
 
     // SkMulDiv255Round() applied to each lane.
@@ -114,6 +118,16 @@
         return vqadd_u8(src, SkMulDiv255Round_neon8(nalphas, dst));
     }
 
+#else // __ARM_64BIT_STATE
+// Inline ARM AArch32 assembly version
+
+// Macros to specify instructions to only include if targeting ARM or Thumb instruction sets
+#ifdef __thumb__
+#define A(x)
+#define T(x) x
+#else
+#define A(x) x
+#define T(x)
 #endif
 
 #if SK_CPU_LSX_LEVEL >= SK_CPU_LSX_LEVEL_LASX
@@ -158,6 +172,181 @@
     }
 #endif
 
+// These macros permit optionally-included features to be switched using a parameter to another macro
+#define YES(x) x
+#define NO(x)
+
+// How far ahead (pixels) to preload (undefine to disable prefetch) - determined empirically
+#undef PREFETCH_DISTANCE
+#define PREFETCH_DISTANCE "24"
+
+#ifdef PREFETCH_DISTANCE
+#define IF_PRELOAD YES
+#else
+#define IF_PRELOAD NO
+#endif
+
+/// Macro to load or store 1..7 pixels in growing powers-of-2 in size - suitable for leading pixels
+#define S32A_LOADSTORE_LEADING_7(ls, r0, r1, r2, r3, base, opt)                                       \
+                "tst         %[group_size], #1                                                \n\t"   \
+                opt"                                                                          \n\t"   \
+                "beq         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%["#base"]:32]!          \n\t"   \
+                "1:                                                                           \n\t"   \
+                "lsls        %[tmp], %[group_size], #30                                       \n\t"   \
+                "add         %[tmp], %["#base"], #4                                           \n\t"   \
+                "bpl         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "1:                                                                           \n\t"   \
+                "bcc         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "v"#ls"4.8   {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[7],"#r1"[7],"#r2"[7],"#r3"[7]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "1:                                                                           \n\t"   \
+
+/// Macro to load or store 1..7 pixels in shrink powers-of-2 in size - suitable for trailing pixels
+#define S32A_LOADSTORE_TRAILING_7(ls, r0, r1, r2, r3, base, opt)                                      \
+                "lsls        %[tmp], %[group_size], #30                                       \n\t"   \
+                "add         %[tmp], %["#base"], #4                                           \n\t"   \
+                opt"                                                                          \n\t"   \
+                "bcc         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[0],"#r1"[0],"#r2"[0],"#r3"[0]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[1],"#r1"[1],"#r2"[1],"#r3"[1]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "v"#ls"4.8   {"#r0"[2],"#r1"[2],"#r2"[2],"#r3"[2]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[3],"#r1"[3],"#r2"[3],"#r3"[3]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "1:                                                                           \n\t"   \
+                "bpl         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[4],"#r1"[4],"#r2"[4],"#r3"[4]}, [%["#base"]:32], %[eight] \n\t"   \
+                "v"#ls"4.8   {"#r0"[5],"#r1"[5],"#r2"[5],"#r3"[5]}, [%[tmp]:32], %[eight]     \n\t"   \
+                "1:                                                                           \n\t"   \
+                "tst         %[group_size], #1                                                \n\t"   \
+                "beq         1f                                                               \n\t"   \
+                "v"#ls"4.8   {"#r0"[6],"#r1"[6],"#r2"[6],"#r3"[6]}, [%["#base"]:32]!          \n\t"   \
+                "1:                                                                           \n\t"   \
+
+/// Macro to do testing and "over" compositing of a group of 1..7 32bpp premultiplied ARGB leading or trailing pixels
+#define S32A_OPAQUE_7PIX_PROCESS(loadstore_7, size)                                                   \
+    do {                                                                                              \
+        __asm__ volatile (                                                                            \
+                /* Load the leading/trailing source pixels,                                           \
+                 * after initialising all the unused indexes from the first pixel                     \
+                 * so the all-opaque and all-transparent tests still work */                          \
+                loadstore_7(ld, d0, d1, d2, d3, src,                                                  \
+                "vld4.8      {d0[],d1[],d2[],d3[]}, [%[src]]")                                        \
+                /* Test for all-opaque or all-transparent */                                          \
+                "vmov        %[alo], s6                                                       \n\t"   \
+                "vmov        %[ahi], s7                                                       \n\t"   \
+                "vmvn        d31, d3                                                          \n\t"   \
+                "orrs        %[tmp], %[alo], %[ahi]                                           \n\t"   \
+                "it          eq                                                               \n\t"   \
+                "addeq       %[dst], %[group_size], lsl #2                                    \n\t"   \
+                "beq         9f                                                               \n\t"   \
+                "cmp         %[alo], #-1                                                      \n\t"   \
+                "it          eq                                                               \n\t"   \
+                "cmpeq       %[ahi], #-1                                                      \n\t"   \
+                "beq         5f                                                               \n\t"   \
+                /* Translucency used, or a mixture of opaque and transparent */                       \
+                loadstore_7(ld, d20, d21, d22, d23, dst, )                                            \
+                "sub         %[dst], %[group_size], lsl #2                                    \n\t"   \
+                S32A_OPAQUE_8PIX_BLEND(, , q0, q1,, NO, NO, NO)                                       \
+                "5:                                                                           \n\t"   \
+                loadstore_7(st, d0, d1, d2, d3, dst, )                                                \
+                /* Drop through */                                                                    \
+                "9:                                                                           \n\t"   \
+        : /* Outputs */                                                                               \
+                [alo]"=&r"(alo),                                                                      \
+                [ahi]"=&r"(ahi),                                                                      \
+                [tmp]"=&r"(tmp),                                                                      \
+                [src]"+r"(src),                                                                       \
+                [dst]"+r"(dst)                                                                        \
+        : /* Inputs */                                                                                \
+                [group_size]"r"(size),                                                                \
+                     [eight]"r"(eight)                                                                \
+        : /* Clobbers */                                                                              \
+                "cc", "memory"                                                                        \
+        );                                                                                            \
+    } while (0)
+
+/// Macro to do testing and "over" compositing of an aligned group of 8 32bpp premultiplied ARGB leading or trailing pixels
+#define S32A_OPAQUE_8PIX_PROCESS(align, if_load)                                                      \
+    do {                                                                                              \
+        __asm__ (                                                                                     \
+if_load(        "vld4.8      {d0-d3}, [%[src]]!                                               \n\t")  \
+                /* Test for all-opaque or all-transparent */                                          \
+                "vmov        %[alo], s6                                                       \n\t"   \
+                "vmov        %[ahi], s7                                                       \n\t"   \
+                "vmvn        d31, d3                                                          \n\t"   \
+                "orrs        %[tmp], %[alo], %[ahi]                                           \n\t"   \
+                "it          eq                                                               \n\t"   \
+                "addeq       %[dst], #8*4                                                     \n\t"   \
+                "beq         9f                                                               \n\t"   \
+                "cmp         %[alo], #-1                                                      \n\t"   \
+                "it          eq                                                               \n\t"   \
+                "cmpeq       %[ahi], #-1                                                      \n\t"   \
+                "beq         5f                                                               \n\t"   \
+                /* Translucency used, or a mixture of opaque and transparent */                       \
+                S32A_OPAQUE_8PIX_BLEND(align, , q0, q1, "5:", YES, NO, NO)                            \
+                /* Drop through */                                                                    \
+                "9:                                                                           \n\t"   \
+        :  /* Outputs */                                                                              \
+                [alo]"=&r"(alo),                                                                      \
+                [ahi]"=&r"(ahi),                                                                      \
+                [tmp]"=&r"(tmp),                                                                      \
+                [src]"+r"(src),                                                                       \
+                [dst]"+r"(dst)                                                                        \
+        : /* Inputs */                                                                                \
+        : /* Clobbers */                                                                              \
+                "cc", "memory"                                                                        \
+        );                                                                                            \
+    } while (0)
+
+/// Macro to do "over" compositing blending on 8 32bpp premultiplied ARGB pixels
+/// which are with either translucent or a mixture of opaque and transparent.
+/// Relies on A(x) to determine whether to emit code in ARM state (as opposed to Thumb state).
+/// @arg align           bit-alignment specifier on destination loads/stores (optional)
+/// @arg other_src_alpha D-register specifier for alpha source in other bank (only IF_OVERLAP)
+/// @arg src0            Q-register specifier for blue/green source in this bank
+/// @arg src1            Q-register specifier for red/alpha source in this bank
+/// @arg opt             optional instruction to emit
+/// @arg if_loadstore    YES or NO: whether to do load/store
+/// @arg if_overlap      YES or NO: whether to interleave processing of next iteration
+/// @arg if_preload      YES or NO: whether to insert prefetch instructions
+#define S32A_OPAQUE_8PIX_BLEND(align, other_src_alpha, src0, src1, opt, if_loadstore, if_overlap, if_preload) \
+if_loadstore(   "vld4.8      {d20-d23}, [%[dst]"#align"]                                      \n\t")  \
+if_preload(     "sub         %[tmp], %[len], #1                                               \n\t")  \
+if_overlap(     "vmov        %[alo], %[ahi], "#other_src_alpha"                               \n\t")  \
+if_preload(     "cmp         %[tmp], #" PREFETCH_DISTANCE "                                   \n\t")  \
+                "vmull.u8    q8, d20, d31                                                     \n\t"   \
+if_preload(     "it          cs                                                               \n\t")  \
+if_preload(     "movcs       %[tmp], #" PREFETCH_DISTANCE "                                   \n\t")  \
+                "vmull.u8    q9, d21, d31                                                     \n\t"   \
+                "vmull.u8    q10, d22, d31                                                    \n\t"   \
+                "vmull.u8    q11, d23, d31                                                    \n\t"   \
+if_preload(     "pld         [%[src], %[tmp], lsl #2]                                         \n\t")  \
+                "vrshr.u16   q12, q8, #8                                                      \n\t"   \
+if_preload(     "add         %[tmp], #(32+32)/4                                               \n\t")  \
+                "vrshr.u16   q13, q9, #8                                                      \n\t"   \
+                "vrshr.u16   q14, q10, #8                                                     \n\t"   \
+                "vrshr.u16   q15, q11, #8                                                     \n\t"   \
+if_preload(     "pld         [%[dst], %[tmp], lsl #2]                                         \n\t")  \
+                "vraddhn.u16 d16, q8, q12                                                     \n\t"   \
+                "vraddhn.u16 d17, q9, q13                                                     \n\t"   \
+                "vraddhn.u16 d18, q10, q14                                                    \n\t"   \
+                "vraddhn.u16 d19, q11, q15                                                    \n\t"   \
+if_overlap(     "mvn         %[tmp], %[alo]                                                   \n\t")  \
+if_overlap(     "vmvn        d31, "#other_src_alpha"                                          \n\t")  \
+if_overlap(A(   "orr         %[alo], %[ahi]                                                   \n\t")) \
+                "vadd.i8     "#src0", q8                                                      \n\t"   \
+if_overlap(A(   "mvn         %[ahi], %[ahi]                                                   \n\t")) \
+                "vadd.i8     "#src1", q9                                                      \n\t"   \
+                opt"                                                                          \n\t"   \
+if_loadstore(   "vst4.8      {"#src0", "#src1"}, [%[dst]"#align"]!                            \n\t")  \
+
+#endif // __ARM_64BIT_STATE
+#endif // defined(SK_ARM_HAS_NEON)
+
 namespace SK_OPTS_NS {
 
 /*not static*/
@@ -187,6 +376,10 @@ inline void blit_row_s32a_opaque(SkPMCol
 #endif
 
 #if defined(SK_ARM_HAS_NEON)
+#ifdef __ARM_64BIT_STATE
+    // No attempt has been made to adapt the inline assembly version for AArch64
+    // so fall back to the less performant version that uses intrinsics instead
+
     while (len >= 8) {
         vst4_u8((uint8_t*)dst, SkPMSrcOver_neon8(vld4_u8((const uint8_t*)dst),
                                                  vld4_u8((const uint8_t*)src)));
@@ -209,6 +402,154 @@ inline void blit_row_s32a_opaque(SkPMCol
         vst1_lane_u32(dst, vreinterpret_u32_u8(result), 0);
     }
     return;
+
+#else // __ARM_64BIT_STATE
+    // Inline ARM AArch32 assembly version
+    uint32_t tmp, alo, ahi;
+    const int eight = 8;
+    if (len < 15) {
+        // Too short to attempt aligned processing
+        if (len & 8)
+            S32A_OPAQUE_8PIX_PROCESS(, YES);
+        if (len & 7)
+            S32A_OPAQUE_7PIX_PROCESS(S32A_LOADSTORE_TRAILING_7, len & 7);
+    } else {
+        // The last 8 - 15 pixels (starting from a 4-pixel boundary) are handled together
+        uintptr_t startrup = (uintptr_t) dst / sizeof (*dst) + 3;
+        uintptr_t end = (uintptr_t) dst / sizeof (*dst) + len;
+        size_t trailing;
+        if ((end & 3) == 0)
+            // No blocks of <8 pixels used at end in these cases
+            trailing = 8;
+        else
+            // If length (discounting alignment to 4-pixel boundaries) is an odd number of 4-pixels,
+            // assign this to trailing end to avoid possibility of a leading run of exactly 4
+            trailing = 8 + ((startrup ^ end) & 4) + (end & 3);
+        // The inner loop handles an integer number (0+) of 16-pixel blocks at 4-pixel boundaries
+        // The 0..15 pixels leading up to this are handled together
+        size_t leading8 = (len - trailing) & 8;
+        size_t leading7 = (len - trailing) & 7;
+
+        // Do leading pixels
+        if (leading7 != 0) {
+            len -= leading7;
+            S32A_OPAQUE_7PIX_PROCESS(S32A_LOADSTORE_LEADING_7, leading7);
+        }
+        if (leading8 != 0) {
+            len -= 8;
+            S32A_OPAQUE_8PIX_PROCESS(:128, YES);
+        }
+
+        // Do inner loop
+        __asm__ (
+                // We enter and leave each iteration of the inner loop with the source
+                // pointer 8 pixels ahead and the destination pointer 8 pixels behind
+                // in order to permit good pipelining. The count of remaining pixels is
+                // reduced by 16 to allow the loop termination test to be combined with
+                // the decrementing of the remaining length.
+                "sub         %[dst], #8*4                                                     \n\t"
+                "vld4.8      {d0-d3}, [%[src]]!                                               \n\t"
+                "subs        %[len], #16                                                      \n\t"
+                "bcc         49f                                                              \n\t"
+
+                "10:                                                                          \n\t"
+                // Move alpha to ARM registers for comparison
+                "vmov        %[alo], s6                                                       \n\t"
+                "vmov        %[ahi], s7                                                       \n\t"
+                // Fetch source data for next iteration
+                "vld4.8      {d4-d7}, [%[src]]!                                               \n\t"
+                "add         %[dst], #8*4                                                     \n\t"
+                // Test if all source pixels are transparent (alpha=0)
+                "orrs        %[tmp], %[alo], %[ahi]                                           \n\t"
+                "beq         19f                                                              \n\t"
+                // Find inverse alpha in case full blending required
+                "vmvn        d31, d3                                                          \n\t"
+                // Test if all source pixels are opaque (alpha=0xff)
+                "cmp         %[alo], #-1                                                      \n\t"
+                "it          eq                                                               \n\t"
+                "cmpeq       %[ahi], #-1                                                      \n\t"
+                "bne         30f                                                              \n\t"
+                // Opaque case: copy source to destination
+                "vst4.8      {d0-d3}, [%[dst]:128]                                            \n\t"
+                // Drop through
+                "19:                                                                          \n\t"
+
+                // Move alpha to ARM registers for comparison
+                "vmov        %[alo], s14                                                      \n\t"
+                "vmov        %[ahi], s15                                                      \n\t"
+                // Fetch source data for next iteration
+                "vld4.8      {d0-d3}, [%[src]]!                                               \n\t"
+                "add         %[dst], #8*4                                                     \n\t"
+                // Test if all source pixels are transparent (alpha=0)
+                "orrs        %[tmp], %[alo], %[ahi]                                           \n\t"
+                "beq         29f                                                              \n\t"
+                // Find inverse alpha in case full blending required
+                "vmvn        d31, d7                                                          \n\t"
+                // Test if all source pixels are opaque (alpha=0xff)
+                "cmp         %[alo], #-1                                                      \n\t"
+                "it          eq                                                               \n\t"
+                "cmpeq       %[ahi], #-1                                                      \n\t"
+                "bne         40f                                                              \n\t"
+                // Opaque case: copy source to destination
+                "vst4.8      {d4-d7}, [%[dst]:128]                                            \n\t"
+                // Drop through
+                "29:                                                                          \n\t"
+                "subs        %[len], #16                                                      \n\t"
+                "bcs         10b                                                              \n\t"
+                "b           49f                                                              \n\t"
+
+                // Mixed or translucent pixels in d0-d3
+                "30:                                                                          \n\t"
+                S32A_OPAQUE_8PIX_BLEND(:128, d7, q0, q1,, YES, YES, IF_PRELOAD)
+A(              "teq         %[alo], #0                                                       \n\t")
+T(              "orrs        %[alo], %[alo], %[ahi]                                           \n\t")
+                "vld4.8      {d0-d3}, [%[src]]!                                               \n\t"
+                "beq         29b                                                              \n\t"
+A(              "orrs        %[tmp], %[tmp], %[ahi]                                           \n\t")
+T(              "orns        %[tmp], %[tmp], %[ahi]                                           \n\t")
+                "bne         40f                                                              \n\t"
+                "vst4.8      {d4-d7}, [%[dst]:128]                                            \n\t"
+                "b           29b                                                              \n\t"
+
+                // Mixed or translucent pixels in d4-d7
+                "40:                                                                          \n\t"
+                S32A_OPAQUE_8PIX_BLEND(:128, d3, q2, q3, \
+                "subs        %[len], #16", YES, YES, NO)
+                "bcc         50f                                                              \n\t"
+A(              "teq         %[alo], #0                                                       \n\t")
+T(              "orrs        %[alo], %[alo], %[ahi]                                           \n\t")
+                "vld4.8      {d4-d7}, [%[src]]!                                               \n\t"
+                "beq         19b                                                              \n\t"
+A(              "orrs        %[tmp], %[tmp], %[ahi]                                           \n\t")
+T(              "orns        %[tmp], %[tmp], %[ahi]                                           \n\t")
+                "bne         30b                                                              \n\t"
+                "vst4.8      {d0-d3}, [%[dst]:128]                                            \n\t"
+                "b           19b                                                              \n\t"
+
+                "49:                                                                          \n\t"
+                "add         %[dst], #8*4                                                     \n\t"
+                "50:                                                                          \n\t"
+        : // Outputs
+                [dst]"+r"(dst),
+                [src]"+r"(src),
+                [len]"+r"(len),
+                [alo]"+r"(alo),
+                [ahi]"+r"(ahi),
+                [tmp]"+r"(tmp)
+        : // Inputs
+        : // Clobbers
+                "cc", "memory"
+        );
+
+        // Do trailing pixels.
+        // There will always be more than 8 of these, and the first 8 are already in d0-d3
+        S32A_OPAQUE_8PIX_PROCESS(:128, NO);
+        if (len & 7)
+            S32A_OPAQUE_7PIX_PROCESS(S32A_LOADSTORE_TRAILING_7, len & 7);
+    }
+    return;
+
+#endif // __ARM_64BIT_STATE
 #endif
 
 #if SK_CPU_LSX_LEVEL >= SK_CPU_LSX_LEVEL_LASX
